#!/usr/bin/env python
# coding: utf-8
"""
æ¨¡å‹è®­ç»ƒç»“æœåˆ†æä¸æ¯”è¾ƒå·¥å…·

åŠŸèƒ½ï¼š
1. åŠ è½½å¤šä¸ª Optuna è®­ç»ƒè¿è¡Œçš„ç»“æœ
2. æ¯”è¾ƒä¸åŒè¿è¡Œçš„æ€§èƒ½æŒ‡æ ‡
3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
4. è¾“å‡ºä¸­æ–‡æ‘˜è¦æŠ¥å‘Š

ç”¨æ³•ï¼š
    python analyze_runs.py --results-dir /path/to/optuna_results
    python analyze_runs.py --runs run_20251201_160807 run_20251201_215750
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import warnings

warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“
try:
    import pandas as pd
    import numpy as np
except ImportError as e:
    print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ {e}ã€‚è¯·è¿è¡Œ: pip install pandas numpy")
    sys.exit(1)

try:
    import optuna
except ImportError:
    optuna = None
    print("è­¦å‘Š: æœªå®‰è£… optunaï¼Œéƒ¨åˆ†åŠŸèƒ½å—é™ã€‚")

try:
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    # ä¸­æ–‡å­—ä½“é…ç½®
    zh_fonts = ['Noto Sans CJK SC', 'Noto Serif CJK SC', 'WenQuanYi Micro Hei', 
                'AR PL UMing CN', 'SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    for font in zh_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font] + zh_fonts
            break
    plt.rcParams['axes.unicode_minus'] = False
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("è­¦å‘Š: æœªå®‰è£… matplotlibï¼Œè·³è¿‡å¯è§†åŒ–ã€‚")

try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False


class RunAnalyzer:
    """å•ä¸ªè®­ç»ƒè¿è¡Œåˆ†æå™¨"""
    
    # é»˜è®¤ study nameï¼Œå¯ä»¥é€šè¿‡ç±»å˜é‡ä¿®æ”¹
    DEFAULT_STUDY_NAME = "catboost_stock_3class_v16"
    
    def __init__(self, run_path: str, study_name: str = None):
        self.run_path = run_path
        self.run_name = os.path.basename(run_path)
        self.study_name = study_name or self.DEFAULT_STUDY_NAME
        self.study = None
        self.trials_df = None
        self.best_params = None
        self.metrics = {}
        
    def load(self) -> bool:
        """åŠ è½½è¿è¡Œæ•°æ®"""
        try:
            # 1. å°è¯•åŠ è½½ Optuna æ•°æ®åº“
            db_path = os.path.join(self.run_path, "optuna_catboost_study.db")
            if os.path.exists(db_path) and optuna:
                storage_url = f"sqlite:///{db_path}"
                try:
                    self.study = optuna.load_study(
                        study_name=self.study_name,
                        storage=storage_url
                    )
                except Exception:
                    # å°è¯•å…¶ä»– study name
                    studies = optuna.get_all_study_names(storage_url)
                    if studies:
                        self.study = optuna.load_study(
                            study_name=studies[0],
                            storage=storage_url
                        )
            
            # 2. åŠ è½½è¯•éªŒ CSV æŠ¥å‘Š
            csv_path = os.path.join(self.run_path, "optuna_trials_report_catboost.csv")
            if os.path.exists(csv_path):
                self.trials_df = pd.read_csv(csv_path)
            
            # 3. åŠ è½½æœ€ä½³å‚æ•°
            params_path = os.path.join(self.run_path, "final_model_params.json")
            if os.path.exists(params_path):
                with open(params_path, 'r') as f:
                    self.best_params = json.load(f)
            
            # 4. æå–åˆ†ç±»æŠ¥å‘Š
            report_path = os.path.join(self.run_path, "classification_report_detailed.csv")
            if os.path.exists(report_path):
                self.metrics['classification_report'] = pd.read_csv(report_path, index_col=0)
            
            # 5. æå–å…³é”®æŒ‡æ ‡
            self._extract_metrics()
            
            return True
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½ {self.run_name} æ—¶å‡ºé”™: {e}")
            return False
    
    def _extract_metrics(self):
        """ä»å·²åŠ è½½æ•°æ®æå–å…³é”®æŒ‡æ ‡"""
        # ä» study æå–
        if self.study and self.study.best_trial:
            self.metrics['best_value'] = self.study.best_value
            self.metrics['n_trials'] = len(self.study.trials)
            self.metrics['n_completed'] = len([t for t in self.study.trials 
                                               if t.state == optuna.trial.TrialState.COMPLETE])
            self.metrics['n_pruned'] = len([t for t in self.study.trials 
                                            if t.state == optuna.trial.TrialState.PRUNED])
            self.metrics['best_params'] = self.study.best_params
        
        # ä» CSV æå–ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        elif self.trials_df is not None and not self.trials_df.empty:
            self.metrics['best_value'] = self.trials_df['value'].max()
            self.metrics['n_trials'] = len(self.trials_df)
            if 'state' in self.trials_df.columns:
                self.metrics['n_completed'] = (self.trials_df['state'] == 'COMPLETE').sum()
                self.metrics['n_pruned'] = (self.trials_df['state'] == 'PRUNED').sum()
        
        # ä» best_params JSON
        if self.best_params:
            self.metrics['vol_multiplier'] = self.best_params.get('vol_multiplier_best')
            self.metrics['offset'] = self.best_params.get('offset_best')
            self.metrics['mode'] = self.best_params.get('mode')
        
        # è§£æè¿è¡Œæ—¶é—´æˆ³
        try:
            if self.run_name.startswith('run_'):
                ts = self.run_name.replace('run_', '')
                self.metrics['timestamp'] = datetime.strptime(ts, '%Y%m%d_%H%M%S')
        except Exception:
            pass
    
    def get_summary(self) -> Dict:
        """è¿”å›è¿è¡Œæ‘˜è¦"""
        return {
            'run_name': self.run_name,
            'run_path': self.run_path,
            **self.metrics
        }


class MultiRunComparator:
    """å¤šè¿è¡Œæ¯”è¾ƒå™¨"""
    
    def __init__(self, results_dir: str = None, run_names: List[str] = None, study_name: str = None):
        self.results_dir = results_dir
        self.run_names = run_names or []
        self.study_name = study_name
        self.analyzers: List[RunAnalyzer] = []
        self.comparison_df = None
        
    def discover_runs(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å¯ç”¨çš„è¿è¡Œç›®å½•"""
        if not self.results_dir or not os.path.exists(self.results_dir):
            print(f"è­¦å‘Š: ç»“æœç›®å½•ä¸å­˜åœ¨: {self.results_dir}")
            return []
        
        runs = []
        for name in os.listdir(self.results_dir):
            run_path = os.path.join(self.results_dir, name)
            if os.path.isdir(run_path) and name.startswith('run_'):
                runs.append(name)
        
        # æŒ‰æ—¶é—´æ’åº
        runs.sort(reverse=True)
        return runs
    
    def load_runs(self, run_names: List[str] = None):
        """åŠ è½½æŒ‡å®šçš„è¿è¡Œ"""
        if run_names:
            self.run_names = run_names
        
        if not self.run_names:
            self.run_names = self.discover_runs()
        
        if not self.run_names:
            print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒè¿è¡Œè®°å½•ã€‚")
            return
        
        print(f"\næ­£åœ¨åŠ è½½ {len(self.run_names)} ä¸ªè®­ç»ƒè¿è¡Œ...")
        
        for name in self.run_names:
            if self.results_dir:
                run_path = os.path.join(self.results_dir, name)
            else:
                run_path = name  # å‡è®¾ä¼ å…¥çš„æ˜¯å®Œæ•´è·¯å¾„
            
            if not os.path.exists(run_path):
                print(f"  è·³è¿‡: {name} (ç›®å½•ä¸å­˜åœ¨)")
                continue
            
            analyzer = RunAnalyzer(run_path, study_name=self.study_name)
            if analyzer.load():
                self.analyzers.append(analyzer)
                print(f"  âœ“ å·²åŠ è½½: {name}")
            else:
                print(f"  âœ— åŠ è½½å¤±è´¥: {name}")
        
        print(f"\næˆåŠŸåŠ è½½ {len(self.analyzers)} ä¸ªè¿è¡Œã€‚")
    
    def compare(self) -> pd.DataFrame:
        """æ¯”è¾ƒæ‰€æœ‰è¿è¡Œ"""
        if not self.analyzers:
            print("é”™è¯¯: æ²¡æœ‰å·²åŠ è½½çš„è¿è¡Œæ•°æ®ã€‚")
            return pd.DataFrame()
        
        summaries = [a.get_summary() for a in self.analyzers]
        self.comparison_df = pd.DataFrame(summaries)
        
        # æŒ‰ best_value æ’åº
        if 'best_value' in self.comparison_df.columns:
            self.comparison_df = self.comparison_df.sort_values(
                'best_value', ascending=False
            ).reset_index(drop=True)
        
        return self.comparison_df
    
    def print_summary(self):
        """æ‰“å°æ¯”è¾ƒæ‘˜è¦"""
        if self.comparison_df is None:
            self.compare()
        
        if self.comparison_df.empty:
            print("æ²¡æœ‰å¯ç”¨çš„æ¯”è¾ƒæ•°æ®ã€‚")
            return
        
        print("\n" + "=" * 70)
        print("                    æ¨¡å‹è®­ç»ƒç»“æœåˆ†ææŠ¥å‘Š")
        print("=" * 70)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"åˆ†æè¿è¡Œæ•°: {len(self.comparison_df)}")
        print("=" * 70)
        
        # æœ€ä½³è¿è¡Œ
        if 'best_value' in self.comparison_df.columns:
            best_idx = self.comparison_df['best_value'].idxmax()
            best_run = self.comparison_df.iloc[best_idx]
            
            print("\nğŸ“Š æœ€ä½³è¿è¡Œ")
            print("-" * 40)
            print(f"  è¿è¡Œåç§°: {best_run['run_name']}")
            print(f"  æœ€ä½³å¾—åˆ†: {best_run['best_value']:.6f}")
            if 'n_trials' in best_run:
                print(f"  è¯•éªŒæ¬¡æ•°: {best_run.get('n_trials', 'N/A')}")
            if 'vol_multiplier' in best_run and pd.notna(best_run['vol_multiplier']):
                print(f"  æ³¢åŠ¨ç‡ä¹˜æ•°: {best_run['vol_multiplier']:.4f}")
        
        # è¿è¡Œåˆ—è¡¨
        print("\nğŸ“‹ æ‰€æœ‰è¿è¡Œæ’å (æŒ‰å¾—åˆ†é™åº)")
        print("-" * 70)
        print(f"{'æ’å':<5} {'è¿è¡Œåç§°':<25} {'æœ€ä½³å¾—åˆ†':<12} {'è¯•éªŒæ•°':<10} {'å®Œæˆç‡':<10}")
        print("-" * 70)
        
        for idx, row in self.comparison_df.iterrows():
            rank = idx + 1
            name = row['run_name'][:24] if len(row['run_name']) > 24 else row['run_name']
            score = f"{row.get('best_value', 0):.6f}" if pd.notna(row.get('best_value')) else "N/A"
            n_trials = row.get('n_trials', 'N/A')
            n_completed = row.get('n_completed', 0)
            if n_trials and n_trials != 'N/A' and n_trials > 0:
                completion_rate = f"{n_completed / n_trials * 100:.1f}%"
            else:
                completion_rate = "N/A"
            
            print(f"{rank:<5} {name:<25} {score:<12} {str(n_trials):<10} {completion_rate:<10}")
        
        print("-" * 70)
        
        # ç»Ÿè®¡æ‘˜è¦
        if 'best_value' in self.comparison_df.columns:
            values = self.comparison_df['best_value'].dropna()
            if len(values) > 0:
                print("\nğŸ“ˆ å¾—åˆ†ç»Ÿè®¡")
                print("-" * 40)
                print(f"  æœ€é«˜åˆ†: {values.max():.6f}")
                print(f"  æœ€ä½åˆ†: {values.min():.6f}")
                print(f"  å¹³å‡åˆ†: {values.mean():.6f}")
                print(f"  æ ‡å‡†å·®: {values.std():.6f}")
        
        print("\n" + "=" * 70)
    
    def plot_comparison(self, save_path: str = None):
        """ç”Ÿæˆæ¯”è¾ƒå¯è§†åŒ–"""
        if not HAS_MATPLOTLIB:
            print("è·³è¿‡å¯è§†åŒ–: matplotlib æœªå®‰è£…ã€‚")
            return
        
        if self.comparison_df is None or self.comparison_df.empty:
            print("æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–ã€‚")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('æ¨¡å‹è®­ç»ƒç»“æœæ¯”è¾ƒåˆ†æ', fontsize=14, fontweight='bold')
        
        # 1. æœ€ä½³å¾—åˆ†å¯¹æ¯”æŸ±çŠ¶å›¾
        ax1 = axes[0, 0]
        if 'best_value' in self.comparison_df.columns:
            df_plot = self.comparison_df.dropna(subset=['best_value']).head(10)
            if not df_plot.empty:
                colors = ['#2ecc71' if i == 0 else '#3498db' for i in range(len(df_plot))]
                bars = ax1.barh(df_plot['run_name'], df_plot['best_value'], color=colors)
                ax1.set_xlabel('æœ€ä½³å¾—åˆ† (Macro F1 * Balance)')
                ax1.set_title('å„è¿è¡Œæœ€ä½³å¾—åˆ†å¯¹æ¯”')
                ax1.invert_yaxis()
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾ - ä½¿ç”¨ç›¸å¯¹åç§»é‡
                max_val = df_plot['best_value'].max()
                min_val = df_plot['best_value'].min()
                offset = (max_val - min_val) * 0.02 if max_val > min_val else 0.005
                for bar, val in zip(bars, df_plot['best_value']):
                    ax1.text(bar.get_width() + offset, bar.get_y() + bar.get_height()/2,
                            f'{val:.4f}', va='center', fontsize=9)
        
        # 2. è¯•éªŒå®Œæˆç‡é¥¼å›¾
        ax2 = axes[0, 1]
        if 'n_completed' in self.comparison_df.columns and 'n_pruned' in self.comparison_df.columns:
            total_completed = int(self.comparison_df['n_completed'].fillna(0).sum())
            total_pruned = int(self.comparison_df['n_pruned'].fillna(0).sum())
            if 'n_trials' in self.comparison_df.columns:
                total_trials = int(self.comparison_df['n_trials'].fillna(0).sum())
            else:
                total_trials = total_completed + total_pruned
            
            if total_trials > 0:
                other = max(0, total_trials - total_completed - total_pruned)
                # è¿‡æ»¤æ‰å¤§å°ä¸º 0 çš„éƒ¨åˆ†
                sizes = []
                labels = []
                colors_list = []
                if total_completed > 0:
                    sizes.append(total_completed)
                    labels.append(f'å®Œæˆ ({total_completed})')
                    colors_list.append('#2ecc71')
                if total_pruned > 0:
                    sizes.append(total_pruned)
                    labels.append(f'å‰ªæ ({total_pruned})')
                    colors_list.append('#e74c3c')
                if other > 0:
                    sizes.append(other)
                    labels.append(f'å…¶ä»– ({other})')
                    colors_list.append('#95a5a6')
                
                if sizes:
                    ax2.pie(sizes, labels=labels, colors=colors_list, autopct='%1.1f%%', startangle=90)
                ax2.set_title('å…¨éƒ¨è¯•éªŒçŠ¶æ€åˆ†å¸ƒ')
        
        # 3. å¾—åˆ†è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
        ax3 = axes[1, 0]
        if 'timestamp' in self.comparison_df.columns and 'best_value' in self.comparison_df.columns:
            df_time = self.comparison_df.dropna(subset=['timestamp', 'best_value']).copy()
            if not df_time.empty:
                df_time = df_time.sort_values('timestamp')
                ax3.plot(df_time['timestamp'], df_time['best_value'], 
                        marker='o', linewidth=2, markersize=8, color='#3498db')
                ax3.fill_between(df_time['timestamp'], df_time['best_value'], alpha=0.3)
                ax3.set_xlabel('è¿è¡Œæ—¶é—´')
                ax3.set_ylabel('æœ€ä½³å¾—åˆ†')
                ax3.set_title('å¾—åˆ†éšæ—¶é—´å˜åŒ–è¶‹åŠ¿')
                ax3.tick_params(axis='x', rotation=45)
        else:
            ax3.text(0.5, 0.5, 'æ— æ—¶é—´åºåˆ—æ•°æ®', ha='center', va='center', fontsize=12)
            ax3.set_title('å¾—åˆ†è¶‹åŠ¿')
        
        # 4. å‚æ•°åˆ†å¸ƒï¼ˆå¦‚æœæœ‰ vol_multiplierï¼‰
        ax4 = axes[1, 1]
        if 'vol_multiplier' in self.comparison_df.columns:
            vol_data = self.comparison_df['vol_multiplier'].dropna()
            if len(vol_data) > 0:
                if HAS_SEABORN:
                    sns.histplot(vol_data, kde=True, ax=ax4, color='#9b59b6')
                else:
                    ax4.hist(vol_data, bins=10, color='#9b59b6', edgecolor='white')
                ax4.set_xlabel('æ³¢åŠ¨ç‡ä¹˜æ•°')
                ax4.set_ylabel('é¢‘æ¬¡')
                ax4.set_title('æœ€ä½³æ³¢åŠ¨ç‡ä¹˜æ•°åˆ†å¸ƒ')
        else:
            ax4.text(0.5, 0.5, 'æ— å‚æ•°æ•°æ®', ha='center', va='center', fontsize=12)
            ax4.set_title('å‚æ•°åˆ†å¸ƒ')
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"âœ“ æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def export_report(self, output_path: str):
        """å¯¼å‡ºè¯¦ç»†æŠ¥å‘Šä¸º CSV"""
        if self.comparison_df is None:
            self.compare()
        
        if self.comparison_df.empty:
            print("æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®ã€‚")
            return
        
        self.comparison_df.to_csv(output_path, index=False)
        print(f"âœ“ æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def get_best_run(self) -> Optional[RunAnalyzer]:
        """è¿”å›æœ€ä½³è¿è¡Œçš„åˆ†æå™¨"""
        if not self.analyzers:
            return None
        
        if self.comparison_df is None:
            self.compare()
        
        if 'best_value' not in self.comparison_df.columns:
            return self.analyzers[0] if self.analyzers else None
        
        best_name = self.comparison_df.iloc[0]['run_name']
        for analyzer in self.analyzers:
            if analyzer.run_name == best_name:
                return analyzer
        
        return None


def main():
    parser = argparse.ArgumentParser(
        description='æ¨¡å‹è®­ç»ƒç»“æœåˆ†æä¸æ¯”è¾ƒå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰è¿è¡Œ
  python analyze_runs.py --results-dir ~/gdrive/optuna_results

  # åˆ†æç‰¹å®šçš„è¿è¡Œ
  python analyze_runs.py --results-dir ~/gdrive/optuna_results \\
      --runs run_20251201_160807 run_20251201_215750

  # å¯¼å‡ºæŠ¥å‘Šå’Œå›¾è¡¨
  python analyze_runs.py --results-dir ~/gdrive/optuna_results \\
      --output-csv comparison.csv --output-plot comparison.png
        """
    )
    
    parser.add_argument(
        '--results-dir', '-d',
        type=str,
        help='Optuna ç»“æœæ ¹ç›®å½• (åŒ…å« run_* å­ç›®å½•)'
    )
    
    parser.add_argument(
        '--runs', '-r',
        nargs='+',
        type=str,
        help='è¦åˆ†æçš„ç‰¹å®šè¿è¡Œåç§° (å¦‚ run_20251201_160807)'
    )
    
    parser.add_argument(
        '--output-csv', '-o',
        type=str,
        help='è¾“å‡ºæ¯”è¾ƒ CSV æŠ¥å‘Šçš„è·¯å¾„'
    )
    
    parser.add_argument(
        '--output-plot', '-p',
        type=str,
        help='è¾“å‡ºæ¯”è¾ƒå›¾è¡¨çš„è·¯å¾„'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºé”™è¯¯'
    )
    
    parser.add_argument(
        '--study-name', '-s',
        type=str,
        default=None,
        help='Optuna study åç§° (é»˜è®¤: catboost_stock_3class_v16)'
    )
    
    args = parser.parse_args()
    
    # é»˜è®¤ç»“æœç›®å½•
    if not args.results_dir:
        # å°è¯•å¸¸è§ä½ç½®
        possible_dirs = [
            os.path.expanduser('~/gdrive/optuna_results'),
            os.path.expanduser('~/optuna_results'),
            '/mnt/workspace/optuna_results',
            '/content/drive/MyDrive/Colab Notebooks/optuna_results'
        ]
        for d in possible_dirs:
            if os.path.exists(d):
                args.results_dir = d
                break
    
    if not args.results_dir or not os.path.exists(args.results_dir):
        print("é”™è¯¯: è¯·ä½¿ç”¨ --results-dir æŒ‡å®šæœ‰æ•ˆçš„ç»“æœç›®å½•ã€‚")
        print("\nä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚")
        sys.exit(1)
    
    # åˆ›å»ºæ¯”è¾ƒå™¨
    comparator = MultiRunComparator(results_dir=args.results_dir, study_name=args.study_name)
    comparator.load_runs(args.runs)
    
    if not comparator.analyzers:
        print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•è®­ç»ƒè¿è¡Œæ•°æ®ã€‚")
        sys.exit(1)
    
    # æ¯”è¾ƒ
    comparator.compare()
    
    # è¾“å‡ºæ‘˜è¦
    if not args.quiet:
        comparator.print_summary()
    
    # å¯¼å‡º CSV
    if args.output_csv:
        comparator.export_report(args.output_csv)
    
    # ç”Ÿæˆå›¾è¡¨
    if args.output_plot:
        comparator.plot_comparison(args.output_plot)
    elif HAS_MATPLOTLIB and not args.quiet:
        # é»˜è®¤ä¿å­˜åˆ°ç»“æœç›®å½•
        default_plot_path = os.path.join(args.results_dir, 'runs_comparison.png')
        comparator.plot_comparison(default_plot_path)
    
    # è¿”å›æœ€ä½³è¿è¡Œä¿¡æ¯
    best_run = comparator.get_best_run()
    if best_run and not args.quiet:
        print(f"\næ¨èä½¿ç”¨: {best_run.run_name}")
        if best_run.best_params:
            print("æœ€ä½³å‚æ•°é…ç½®:")
            print(json.dumps(best_run.best_params, indent=2, ensure_ascii=False))


if __name__ == '__main__':
    main()
