module JuliaCore

using Reexport
using PythonCall
using DataFrames
using Parquet2
using Dates
using Printf

# åŒ…å«å„ä¸ªå­æ¨¡å—
include("Shared/Shared.jl")
@reexport using .Shared

include("Training/Training.jl")
@reexport using .Training

include("Prediction/Prediction.jl")
@reexport using .Prediction

# å£°æ˜å°†è¦å¯¼å…¥çš„Pythonæ¨¡å—å¯¹è±¡
const cb = PythonCall.pynew()
const optuna = PythonCall.pynew()
const sklearn_metrics = PythonCall.pynew()

"""
    __init__()

Juliaæ¨¡å—çš„åˆå§‹åŒ–å‡½æ•°ã€‚å½“æ­¤æ¨¡å—è¢«åŠ è½½æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨æ­¤å‡½æ•°ã€‚
å®ƒè´Ÿè´£åˆå§‹åŒ–ä¸Pythonçš„è¿æ¥ï¼Œå¹¶å¯¼å…¥æ‰€éœ€çš„Pythonåº“ï¼ˆå¦‚catboost, optunaï¼‰ã€‚
å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä¼šå‘å‡ºè­¦å‘Šï¼Œä½†ä¸ä¼šä¸­æ–­ç¨‹åºã€‚
"""
function __init__()
    # åˆå§‹åŒ– Python æ¨¡å—
    try
        PythonCall.pycopy!(cb, pyimport("catboost"))
        PythonCall.pycopy!(optuna, pyimport("optuna"))
        PythonCall.pycopy!(sklearn_metrics, pyimport("sklearn.metrics"))
    catch e
        @warn "å¯¼å…¥ Python æ¨¡å—å¤±è´¥: $e"
    end
end

export run_pipeline, load_and_process_data

"""
    load_and_process_data(data_dir::String; limit::Int=0)

ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰Parquetæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸ªæ–‡ä»¶åº”ç”¨ç‰¹å¾å·¥ç¨‹ã€‚

è¿™æ˜¯ä¸€ä¸ªå¤šçº¿ç¨‹å‡½æ•°ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®ï¼Œæå¤§åœ°æé«˜äº†æ•ˆç‡ã€‚

å‚æ•°:
- data_dir: å­˜æ”¾Parquetæ•°æ®æ–‡ä»¶çš„ç›®å½•è·¯å¾„
- limit: å¯é€‰å‚æ•°ï¼Œç”¨äºé™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆå¸¸ç”¨äºè°ƒè¯•ï¼‰

è¿”å›å€¼:
- ä¸€ä¸ªåˆå¹¶äº†æ‰€æœ‰è‚¡ç¥¨ç‰¹å¾çš„å¤§å‹DataFrame

æµç¨‹:
1. éå†ç›®å½•ï¼Œç­›é€‰å‡º.parquetæ–‡ä»¶ã€‚
2. ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆThreads.@threadsï¼‰å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼š
   - è¯»å–Parquetæ–‡ä»¶åˆ°DataFrame
   - ç¡®ä¿åˆ—ç±»å‹æ­£ç¡®
   - è°ƒç”¨apply_technical_indicators!è®¡ç®—ç‰¹å¾
3. å°†æ‰€æœ‰å¤„ç†å¥½çš„DataFrameåˆå¹¶æˆä¸€ä¸ªå¤§è¡¨ã€‚
"""
function load_and_process_data(data_dir::String; limit::Int=0)
    if !isdir(data_dir)
        println("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: $data_dir")
        return DataFrame()
    end

    files = readdir(data_dir, join=true)
    # è¿‡æ»¤ parquet
    files = filter(f -> endswith(f, ".parquet"), files)
    
    if limit > 0
        println("âš ï¸ é™åˆ¶å¤„ç†å‰ $limit ä¸ªæ–‡ä»¶")
        files = files[1:min(limit, length(files))]
    end
    
    println("ğŸš€ å‘ç° $(length(files)) ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¹¶è¡Œå¤„ç†...")

    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„é›†åˆæ¥æ”¶é›† DataFrame
    # åˆå§‹åŒ–ä¸º nothing
    processed_dfs = Vector{Union{DataFrame, Nothing}}(nothing, length(files))

    Threads.@threads for i in 1:length(files)
        try
            # è¯»å–
            ds = Parquet2.readfile(files[i])
            df = DataFrame(ds; copycols=true)
            
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ ‡å‡† Vector (å¤„ç† FillArrays ç­‰ç‰¹æ®Šç±»å‹)
            for col in names(df)
                df[!, col] = collect(df[!, col])
            end
            
            if i == 1
                println("åˆ—å: ", names(df))
            end
            
            # ç‰¹å¾å·¥ç¨‹ (Julia åŸç”Ÿé€Ÿåº¦)
            # æ³¨æ„ï¼šapply_technical_indicators! æ˜¯åŸåœ°ä¿®æ”¹
            apply_technical_indicators!(df)
            
            # ç®€å•çš„è¿‡æ»¤é€»è¾‘
            if nrow(df) > 20
                 processed_dfs[i] = df
            end
        catch e
            println("å¤„ç†æ–‡ä»¶ $(files[i]) æ—¶å‡ºé”™: $e")
            # å¿½ç•¥é”™è¯¯ï¼Œä¿æŒé™é»˜æˆ–è®°å½•æ—¥å¿—
        end
    end

    # æ”¶é›†æœ‰æ•ˆç»“æœ
    valid_dfs = DataFrame[]
    for res in processed_dfs
        if !isnothing(res)
            push!(valid_dfs, res)
        end
    end
    
    # åˆå¹¶
    println("ğŸ“š æ­£åœ¨åˆå¹¶æ•°æ®...")
    if isempty(valid_dfs)
        println("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¢«å¤„ç†ï¼")
        return DataFrame()
    end
    
    # ä½¿ç”¨ cols=:union å…è®¸åˆ—ä¸ä¸€è‡´ (å¡«å…… missing)
    full_df = vcat(valid_dfs..., cols=:union)
    return full_df
end

"""
    run_pipeline(data_dir::String)

è¿è¡Œå®Œæ•´çš„é‡åŒ–åˆ†ææµç¨‹ï¼šæ•°æ®åŠ è½½ -> ç‰¹å¾å·¥ç¨‹ -> æ¨¡å‹è®­ç»ƒã€‚

è¿™æ˜¯é¡¹ç›®çš„ä¸€ä¸ªé«˜å±‚æ¥å£ï¼Œå°†å¤æ‚çš„æµç¨‹å°è£…èµ·æ¥ã€‚

å‚æ•°:
- data_dir: æ•°æ®ç›®å½•è·¯å¾„

è¯´æ˜:
- æ­¤å‡½æ•°é¦–å…ˆè°ƒç”¨load_and_process_dataè·å–å¸¦ç‰¹å¾çš„æ•°æ®ã€‚
- ç„¶åå‡†å¤‡è®­ç»ƒé›†Xå’Œæ ‡ç­¾yã€‚
- æœ€åä½¿ç”¨CatBoostè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶ä¿å­˜æ¨¡å‹æ–‡ä»¶ã€‚
- æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸­çš„æ¨¡å‹è®­ç»ƒæ˜¯ç®€åŒ–çš„æ¼”ç¤ºï¼Œå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨train_modelå‡½æ•°è¿›è¡Œè¶…å‚ä¼˜åŒ–ã€‚
"""
function run_pipeline(data_dir::String)
    # åŠ è½½æ•°æ®
    df = load_and_process_data(data_dir)
    
    if nrow(df) == 0
        return
    end
    
    println("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    # prepare_training_data ç°åœ¨ä¸»è¦è´Ÿè´£æ¸…æ´—å’Œç±»å‹è½¬æ¢
    df_train = prepare_training_data(df)
    
    println("ğŸ¤– å¼€å§‹ Optuna å¯»ä¼˜...")
    # ä½¿ç”¨ Training.Daily.Train ä¸­çš„ train_model
    best_params = train_model(df_train, n_trials=20) # å¢åŠ  trial æ¬¡æ•°
    
    println("âœ… ä¼˜åŒ–å®Œæˆã€‚æœ€ä½³å‚æ•°: $best_params")
    
    # TODO: ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶ä¿å­˜
    # ç›®å‰ train_model å·²ç»åŒ…å«äº†å®Œæ•´çš„ Optuna æµç¨‹
end

end # module
