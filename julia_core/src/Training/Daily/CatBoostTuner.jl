"""
    CatBoostTuner.jl

é’ˆå¯¹ Tesla P4 GPU ä¼˜åŒ–çš„ CatBoost è¶…å‚æ•°æœç´¢æ¨¡å—ã€‚

è®¾è®¡ç†å¿µ:
1. åˆ©ç”¨ CatBoost å¯¹ç§°æ ‘åœ¨ P4 ä¸Šçš„é«˜æ•ˆæ€§ï¼ˆé›¶åˆ†æ”¯å‘æ•£ï¼‰
2. å……åˆ†åˆ©ç”¨ ~5ç§’/è½® çš„è®­ç»ƒé€Ÿåº¦ï¼Œè·‘å¤§è§„æ¨¡å‚æ•°æœç´¢
3. é’ˆå¯¹ P4 çš„ç‰¹ç‚¹ä¼˜åŒ– border_count (ç›´æ–¹å›¾ç®±æ•°)
4. å°æ•°æ®é‡æ—¶è‡ªåŠ¨å›é€€åˆ° CPU (é¿å…ç»Ÿè®¡åå·®å¯¼è‡´çš„ F1 å´©æºƒ)

Tesla P4 ç‰¹æ€§:
- æ¨ç†ä¼˜åŒ–å¡ï¼ŒFP32 ä»… 5.5 TFLOPS
- CatBoost å¯¹ç§°æ ‘å®Œç¾å¥‘åˆï¼Œæ— åˆ†æ”¯å‘æ•£
- å¤§æ•°æ®é‡ (>1ä¸‡è¡Œ) æ—¶ GPU åŠ é€Ÿ 5.3x
- å°æ•°æ®é‡ (<1ä¸‡è¡Œ) æ—¶ GPU ç²¾åº¦ä¸‹é™ï¼Œå»ºè®®ç”¨ CPU
"""
module CatBoostTuner

using DataFrames
using PythonCall
using Statistics
using Random
using Dates
using Printf
using Hyperopt
using Hyperopt: BOHB, Continuous, Categorical

# ä½¿ç”¨å…±äº«æ¨¡å—
using ....Shared.Types: DailyTimeframe, ClassificationTargetConfig
using ....Shared.Features.Daily: apply_technical_indicators!
using ....Shared.Targets: add_future_returns!, create_targets!
using ....Shared.DataFetcher: load_data_files

export run_catboost_grid_search, run_catboost_random_search, 
       run_catboost_hyperopt, run_catboost_bayesian,
       TunerConfig, save_best_model

# ============================================================================
# é…ç½®ç»“æ„
# ============================================================================

"""
    TunerConfig

è¶…å‚æ•°æœç´¢é…ç½®
"""
Base.@kwdef struct TunerConfig
    # æ•°æ®é…ç½®
    data_dir::String = "data/daily"
    limit_files::Union{Int, Nothing} = nothing
    train_ratio::Float64 = 0.8
    
    # è®¾å¤‡é…ç½®
    auto_device::Bool = true              # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    min_samples_for_gpu::Int = 10000      # GPU æœ€å°æ ·æœ¬æ•°é˜ˆå€¼
    force_gpu::Bool = false               # å¼ºåˆ¶ä½¿ç”¨ GPU
    
    # P4 ä¼˜åŒ–å‚æ•°
    border_count_gpu::Int = 64            # GPU æ¨¡å¼ä¸‹çš„ç›´æ–¹å›¾ç®±æ•° (P4 ä¼˜åŒ–)
    border_count_cpu::Int = 128           # CPU æ¨¡å¼ä¸‹çš„ç›´æ–¹å›¾ç®±æ•°
    
    # æœç´¢é…ç½®
    n_trials::Int = 50                    # Random Search è¯•éªŒæ¬¡æ•°
    early_stopping_rounds::Int = 50       # æ—©åœè½®æ•°
    random_seed::Int = 42
    
    # ç›®æ ‡é…ç½®
    vol_multiplier::Float64 = 1.0         # æ³¢åŠ¨ç‡ä¹˜æ•°
end

# ============================================================================
# å‚æ•°ç½‘æ ¼å®šä¹‰ (é’ˆå¯¹ P4 ä¼˜åŒ–)
# ============================================================================

"""
é’ˆå¯¹ Tesla P4 ä¼˜åŒ–çš„å‚æ•°ç½‘æ ¼

å…³é”®æ´å¯Ÿ:
- depth: P4 è·‘æ·±åº¦ 6-8 çš„å¯¹ç§°æ ‘éå¸¸é«˜æ•ˆ
- learning_rate: é…åˆ early_stoppingï¼Œå¯ä»¥ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
- l2_leaf_reg: æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- border_count: P4 ä¸Š 64 é€šå¸¸æ¯” 128/256 æ›´å¿«ä¸”ç²¾åº¦æŸå¤±å°
"""
const P4_OPTIMIZED_GRID = [
    # åŸºå‡†é…ç½®
    (depth=6, lr=0.1, l2=3.0, iters=1000),
    (depth=6, lr=0.08, l2=5.0, iters=1500),
    (depth=6, lr=0.05, l2=3.0, iters=2000),
    
    # æ›´æ·±çš„æ ‘ (P4 å¯¹æ·±åº¦ 8 çš„å¯¹ç§°æ ‘ä¹Ÿå¾ˆé«˜æ•ˆ)
    (depth=8, lr=0.08, l2=5.0, iters=1000),
    (depth=8, lr=0.05, l2=3.0, iters=1500),
    (depth=8, lr=0.03, l2=5.0, iters=2000),
    
    # æ›´æµ…çš„æ ‘ (æ›´å¿«ï¼Œé€‚åˆå¿«é€Ÿç­›é€‰)
    (depth=4, lr=0.15, l2=1.0, iters=1500),
    (depth=4, lr=0.1, l2=3.0, iters=2000),
    
    # é«˜æ­£åˆ™åŒ–é…ç½®
    (depth=6, lr=0.05, l2=10.0, iters=2000),
    (depth=8, lr=0.03, l2=10.0, iters=2500),
    
    # ä½æ­£åˆ™åŒ–é…ç½® (é€‚åˆæ•°æ®é‡å¤§æ—¶)
    (depth=6, lr=0.1, l2=1.0, iters=1000),
    (depth=8, lr=0.08, l2=1.0, iters=1200),
]

"""
æ‰©å±•å‚æ•°ç½‘æ ¼ (ç”¨äºç²¾ç»†è°ƒä¼˜)
"""
const EXTENDED_GRID = [
    # æ·»åŠ æ›´å¤š depth/lr ç»„åˆ
    (depth=5, lr=0.1, l2=3.0, iters=1200),
    (depth=5, lr=0.08, l2=5.0, iters=1500),
    (depth=7, lr=0.08, l2=3.0, iters=1200),
    (depth=7, lr=0.05, l2=5.0, iters=1800),
    (depth=9, lr=0.05, l2=5.0, iters=1500),
    (depth=9, lr=0.03, l2=8.0, iters=2000),
    (depth=10, lr=0.03, l2=10.0, iters=2000),
    
    # æ›´å¤šæ­£åˆ™åŒ–å˜ä½“
    (depth=6, lr=0.08, l2=0.5, iters=1200),
    (depth=6, lr=0.08, l2=8.0, iters=1500),
    (depth=6, lr=0.08, l2=15.0, iters=1800),
    (depth=8, lr=0.05, l2=0.5, iters=1500),
    (depth=8, lr=0.05, l2=15.0, iters=2000),
]

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

"""
    materialize_columns!(df::DataFrame)

å°† DataFrame ä¸­çš„æƒ°æ€§åˆ—è½¬æ¢ä¸ºæ™®é€š Vectorã€‚
"""
function materialize_columns!(df::DataFrame)
    for col in names(df)
        col_data = df[!, col]
        if !(col_data isa Vector)
            df[!, col] = collect(col_data)
        end
    end
end

"""
    prepare_data(config::TunerConfig)

åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚æ”¯æŒä¸¤ç§æ¨¡å¼:
1. å·²æœ‰ç‰¹å¾çš„æ•°æ® (Arrow æ ¼å¼, å« RSI/MACD/SMA ç­‰)
2. åŸå§‹æ•°æ® (éœ€è¦è®¡ç®—ç‰¹å¾)

è¿”å›: (X_train, y_train, X_val, y_val, feature_names, n_samples)
"""
function prepare_data(config::TunerConfig)
    # å¯¼å…¥ Python åº“
    np = pyimport("numpy")
    sklearn_utils_class_weight = pyimport("sklearn.utils.class_weight")
    
    # å®šä¹‰æ—¶é—´æ¡†æ¶å’Œç›®æ ‡é…ç½®
    timeframe = DailyTimeframe()
    target_config = ClassificationTargetConfig(vol_multiplier=config.vol_multiplier)
    
    # åŠ è½½æ•°æ®
    println("ğŸ“‚ ä» $(config.data_dir) åŠ è½½æ•°æ®...")
    raw_dfs = load_data_files(config.data_dir; limit=config.limit_files)
    
    if isempty(raw_dfs)
        error("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
    end
    
    # è¿‡æ»¤ç©ºæ–‡ä»¶
    raw_dfs = filter(df -> nrow(df) > 0 && ncol(df) > 0, raw_dfs)
    
    if isempty(raw_dfs)
        error("æ‰€æœ‰æ•°æ®æ–‡ä»¶éƒ½æ˜¯ç©ºçš„ï¼")
    end
    
    println("   å·²åŠ è½½ $(length(raw_dfs)) ä¸ªæœ‰æ•ˆæ–‡ä»¶")
    
    # æ£€æµ‹æ•°æ®æ˜¯å¦å·²æœ‰ç‰¹å¾
    sample_df = first(raw_dfs)
    has_precomputed_features = all(col -> col in names(sample_df), ["RSI_14", "SMA_20", "MACD_12_26_9"])
    
    if has_precomputed_features
        println("   âœ… æ£€æµ‹åˆ°é¢„è®¡ç®—ç‰¹å¾ï¼Œè·³è¿‡ç‰¹å¾å·¥ç¨‹")
    else
        println("   ğŸ“Š åŸå§‹æ•°æ®ï¼Œå°†è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
    end
    
    # å¤„ç†æ•°æ®
    processed_dfs = DataFrame[]
    
    for df in raw_dfs
        df_copy = copy(df)
        materialize_columns!(df_copy)
        
        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—ç‰¹å¾ï¼Œåˆ™è®¡ç®—
        if !has_precomputed_features
            apply_technical_indicators!(df_copy)
        end
        
        # ç”ŸæˆçœŸæ­£çš„æœªæ¥æ”¶ç›Š (future_return)
        # æ³¨æ„: return_5d æ˜¯è¿‡å»æ”¶ç›Šï¼Œä¸èƒ½ç”¨ï¼
        if "close" in names(df_copy)
            n = nrow(df_copy)
            future_return = Vector{Union{Missing, Float64}}(missing, n)
            
            for i in 1:(n-5)
                if !ismissing(df_copy.close[i]) && !ismissing(df_copy.close[i+5])
                    future_return[i] = df_copy.close[i+5] / df_copy.close[i] - 1.0
                end
            end
            
            df_copy.future_return = future_return
        end
        
        # æ£€æŸ¥ volatility_factor
        if !("volatility_factor" in names(df_copy))
            # å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
            df_copy.volatility_factor = fill(0.02, nrow(df_copy))  # é»˜è®¤ 2% é˜ˆå€¼
        end
        
        # ç”Ÿæˆç›®æ ‡ (å¦‚æœæ²¡æœ‰)
        if !("target" in names(df_copy))
            if "future_return" in names(df_copy) && "volatility_factor" in names(df_copy)
                create_targets!(df_copy, target_config)
            end
        end
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        if "target" in names(df_copy)
            # åˆ é™¤ target ä¸º missing çš„è¡Œ
            df_valid = df_copy[.!ismissing.(df_copy.target), :]
            if nrow(df_valid) > 0
                push!(processed_dfs, df_valid)
            end
        end
    end
    
    if isempty(processed_dfs)
        error("å¤„ç†åæ— æœ‰æ•ˆæ•°æ®ï¼")
    end
    
    # åˆå¹¶æ•°æ®
    full_df = reduce((a, b) -> vcat(a, b; cols=:intersect), processed_dfs)
    
    # å‡†å¤‡ç‰¹å¾ - æ’é™¤éç‰¹å¾åˆ—
    exclude_cols = [
        # æ ‡è¯†åˆ—
        "ts_code", "trade_time", "target", "date",
        # åŸå§‹ä»·æ ¼åˆ—
        "open", "high", "low", "close", "volume", "amount", "pre_close",
        # ç›®æ ‡ç›¸å…³åˆ—
        "future_close", "future_return", "volatility_factor", "pred_5d",
        # æ—¥æœŸåˆ—
        "trade_date", "ann_date", "end_date", "f_ann_date",
        # æ–‡æœ¬/åˆ†ç±»åˆ—
        "trade_status", "crncy_code", "crncy_code_basic",
        # å¤æƒåˆ— (é¿å…æ•°æ®æ³„éœ²)
        "adj_close", "adj_open", "adj_high", "adj_low", "adj_pre_close",
        "adj_factor_x", "adj_factor_y", "adj_factor",
        # å…¶ä»–éæ•°å€¼åˆ—
        "close_basic"
    ]
    feature_cols = setdiff(names(full_df), exclude_cols)
    
    # åªä¿ç•™æ•°å€¼åˆ—
    numeric_cols = String[]
    for col in feature_cols
        col_type = eltype(full_df[!, col])
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹ (åŒ…æ‹¬ Union{Missing, Number})
        if col_type <: Number || (col_type isa Union && any(t -> t <: Number, Base.uniontypes(col_type)))
            push!(numeric_cols, col)
        end
    end
    feature_cols = numeric_cols
    
    println("   ç‰¹å¾åˆ—æ•°: $(length(feature_cols))")
    
    # é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
    select!(full_df, [feature_cols; "target"])
    
    # è½¬æ¢ target åˆ—ç±»å‹å¹¶åˆ é™¤ç¼ºå¤±å€¼
    full_df.target = convert(Vector{Union{Missing, Int}}, full_df.target)
    dropmissing!(full_df)
    
    n_samples = nrow(full_df)
    println("   æ€»æ ·æœ¬æ•°: $n_samples")
    
    if n_samples < 100
        error("æ ·æœ¬æ•°è¿‡å°‘: $n_samples")
    end
    
    X = full_df[:, feature_cols]
    y = Vector{Int}(full_df.target)
    
    # è®­ç»ƒ/éªŒè¯åˆ’åˆ†
    split_idx = floor(Int, n_samples * config.train_ratio)
    
    X_train = X[1:split_idx, :]
    y_train = y[1:split_idx]
    X_val = X[split_idx+1:end, :]
    y_val = y[split_idx+1:end]
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    classes = sort(unique(y_train))
    class_weights = sklearn_utils_class_weight.compute_class_weight(
        "balanced", classes=np.array(classes), y=np.array(y_train)
    )
    weights_dict = pydict(Dict(classes[i] => pyconvert(Float64, class_weights[i-1]) for i in 1:length(classes)))
    
    # å°† DataFrame è½¬æ¢ä¸º Matrixï¼Œå¤„ç† Missing
    X_train_mat = Matrix{Float64}(coalesce.(Matrix(X_train), NaN))
    X_val_mat = Matrix{Float64}(coalesce.(Matrix(X_val), NaN))
    
    # è½¬æ¢ä¸º NumPy
    X_train_np = np.array(X_train_mat)
    y_train_np = np.array(y_train)
    X_val_np = np.array(X_val_mat)
    y_val_np = np.array(y_val)
    
    println("   è®­ç»ƒé›†: $(size(X_train, 1)) æ ·æœ¬")
    println("   éªŒè¯é›†: $(size(X_val, 1)) æ ·æœ¬")
    
    return (
        X_train=X_train_np, 
        y_train=y_train_np, 
        X_val=X_val_np, 
        y_val=y_val_np,
        feature_names=feature_cols,
        n_samples=n_samples,
        class_weights=weights_dict
    )
end

"""
    select_device(n_samples::Int, config::TunerConfig)

æ ¹æ®æ ·æœ¬æ•°è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ã€‚

Tesla P4 ç‰¹æ€§:
- å¤§æ•°æ®é‡ (>1ä¸‡) æ—¶ GPU æœ‰ 5.3x åŠ é€Ÿ
- å°æ•°æ®é‡ (<1ä¸‡) æ—¶ GPU ç²¾åº¦ä¸‹é™ï¼Œå»ºè®®ç”¨ CPU
"""
function select_device(n_samples::Int, config::TunerConfig)
    if config.force_gpu
        return ("GPU", config.border_count_gpu)
    end
    
    if !config.auto_device
        return ("GPU", config.border_count_gpu)
    end
    
    if n_samples >= config.min_samples_for_gpu
        return ("GPU", config.border_count_gpu)
    else
        @warn "æ ·æœ¬æ•° $n_samples < $(config.min_samples_for_gpu)ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° CPU æ¨¡å¼ä»¥ä¿è¯ç²¾åº¦"
        return ("CPU", config.border_count_cpu)
    end
end

"""
    train_and_evaluate(params, data, device_info, config::TunerConfig)

è®­ç»ƒå•ä¸ªæ¨¡å‹å¹¶è¿”å› F1 åˆ†æ•°ã€‚
"""
function train_and_evaluate(params, data, device_info, config::TunerConfig)
    catboost = pyimport("catboost")
    sklearn_metrics = pyimport("sklearn.metrics")
    
    device, border_count = device_info
    
    # æ„å»ºæ¨¡å‹å‚æ•°
    model_kwargs = Dict(
        :iterations => params.iters,
        :depth => params.depth,
        :learning_rate => params.lr,
        :l2_leaf_reg => params.l2,
        :border_count => border_count,
        :loss_function => "MultiClass",
        :eval_metric => "MultiClass",
        :classes_count => 3,
        :class_weights => data.class_weights,
        :verbose => 0,
        :allow_writing_files => false,
        :random_seed => config.random_seed
    )
    
    # è®¾ç½®è®¾å¤‡
    if device == "GPU"
        model_kwargs[:task_type] = "GPU"
        model_kwargs[:devices] = "0"
    else
        model_kwargs[:task_type] = "CPU"
    end
    
    # åˆ›å»ºæ¨¡å‹
    model = catboost.CatBoostClassifier(; model_kwargs...)
    
    # è®­ç»ƒ - ä½¿ç”¨ Pool ä»¥æ”¯æŒç‰¹å¾å
    train_pool = catboost.Pool(data.X_train, label=data.y_train, feature_names=data.feature_names)
    val_pool = catboost.Pool(data.X_val, label=data.y_val, feature_names=data.feature_names)
    
    t_start = time()
    model.fit(
        train_pool,
        eval_set=val_pool,
        early_stopping_rounds=config.early_stopping_rounds
    )
    t_elapsed = time() - t_start
    
    # è¯„ä¼°
    preds = model.predict(data.X_val)
    f1 = pyconvert(Float64, sklearn_metrics.f1_score(data.y_val, preds, average="macro"))
    accuracy = pyconvert(Float64, sklearn_metrics.accuracy_score(data.y_val, preds))
    
    # è·å–å®é™…ä½¿ç”¨çš„è¿­ä»£æ¬¡æ•° (å¯èƒ½å›  early stopping è€Œå‡å°‘)
    best_iter = pyconvert(Int, model.get_best_iteration())
    
    return (f1=f1, accuracy=accuracy, time=t_elapsed, best_iter=best_iter, model=model)
end

# ============================================================================
# ä¸»æœç´¢å‡½æ•°
# ============================================================================

"""
    run_catboost_grid_search(; config=TunerConfig(), extended=false)

è¿è¡Œç½‘æ ¼æœç´¢ã€‚

# Arguments
- `config`: TunerConfig é…ç½®å¯¹è±¡
- `extended`: æ˜¯å¦ä½¿ç”¨æ‰©å±•ç½‘æ ¼

# Returns
- åŒ…å«æœ€ä½³å‚æ•°å’Œæ‰€æœ‰ç»“æœçš„ NamedTuple
"""
function run_catboost_grid_search(; config::TunerConfig=TunerConfig(), extended::Bool=false)
    println("\n" * "="^70)
    println("ğŸš€ CatBoost ç½‘æ ¼æœç´¢ (Tesla P4 ä¼˜åŒ–)")
    println("="^70)
    
    # å‡†å¤‡æ•°æ®
    data = prepare_data(config)
    
    # é€‰æ‹©è®¾å¤‡
    device_info = select_device(data.n_samples, config)
    device, border_count = device_info
    println("\nğŸ“± è®¾å¤‡: $device (border_count=$border_count)")
    
    # é€‰æ‹©ç½‘æ ¼
    grid = extended ? vcat(P4_OPTIMIZED_GRID, EXTENDED_GRID) : P4_OPTIMIZED_GRID
    println("ğŸ“Š å‚æ•°ç»„åˆæ•°: $(length(grid))")
    
    # é¢„ä¼°æ—¶é—´
    est_time_per_trial = device == "GPU" ? 5.0 : 25.0
    est_total_time = est_time_per_trial * length(grid)
    println("â±ï¸  é¢„ä¼°æ€»æ—¶é—´: $(round(est_total_time/60, digits=1)) åˆ†é’Ÿ")
    
    # å¼€å§‹æœç´¢
    results = Vector{NamedTuple}()
    best_f1 = 0.0
    best_params = nothing
    best_model = nothing
    
    println("\n" * "-"^70)
    println("å¼€å§‹æœç´¢...")
    println("-"^70)
    
    for (i, params) in enumerate(grid)
        result = train_and_evaluate(params, data, device_info, config)
        
        push!(results, (
            params=params,
            f1=result.f1,
            accuracy=result.accuracy,
            time=result.time,
            best_iter=result.best_iter
        ))
        
        # æ›´æ–°æœ€ä½³
        status = ""
        if result.f1 > best_f1
            best_f1 = result.f1
            best_params = params
            best_model = result.model
            status = " â­ NEW BEST!"
        end
        
        # æ‰“å°è¿›åº¦
        println(@sprintf("[%2d/%2d] depth=%d, lr=%.3f, l2=%.1f, iters=%d | F1=%.4f, Acc=%.4f, Time=%.1fs%s",
            i, length(grid),
            params.depth, params.lr, params.l2, params.iters,
            result.f1, result.accuracy, result.time, status
        ))
    end
    
    # æ‰“å°æ€»ç»“
    println("\n" * "="^70)
    println("ğŸ“Š æœç´¢å®Œæˆ!")
    println("="^70)
    println("æœ€ä½³ F1: $(round(best_f1, digits=4))")
    println("æœ€ä½³å‚æ•°: depth=$(best_params.depth), lr=$(best_params.lr), l2=$(best_params.l2), iters=$(best_params.iters)")
    println("æ€»ç”¨æ—¶: $(round(sum(r.time for r in results)/60, digits=2)) åˆ†é’Ÿ")
    
    # æ‰“å° Top 5
    sorted_results = sort(results, by=r -> r.f1, rev=true)
    println("\nğŸ† Top 5 é…ç½®:")
    for (i, r) in enumerate(sorted_results[1:min(5, length(sorted_results))])
        println("  #$i: F1=$(round(r.f1, digits=4)), depth=$(r.params.depth), lr=$(r.params.lr), l2=$(r.params.l2)")
    end
    
    return (
        best_f1=best_f1,
        best_params=best_params,
        best_model=best_model,
        all_results=results,
        data=data,
        device=device
    )
end

"""
    run_catboost_random_search(; config=TunerConfig())

è¿è¡Œéšæœºæœç´¢ (æ›´çµæ´»çš„å‚æ•°ç©ºé—´æ¢ç´¢)ã€‚

# Arguments
- `config`: TunerConfig é…ç½®å¯¹è±¡

# Returns
- åŒ…å«æœ€ä½³å‚æ•°å’Œæ‰€æœ‰ç»“æœçš„ NamedTuple
"""
function run_catboost_random_search(; config::TunerConfig=TunerConfig())
    println("\n" * "="^70)
    println("ğŸ² CatBoost éšæœºæœç´¢ (Tesla P4 ä¼˜åŒ–)")
    println("="^70)
    
    # å‡†å¤‡æ•°æ®
    data = prepare_data(config)
    
    # é€‰æ‹©è®¾å¤‡
    device_info = select_device(data.n_samples, config)
    device, border_count = device_info
    println("\nğŸ“± è®¾å¤‡: $device (border_count=$border_count)")
    println("ğŸ“Š è¯•éªŒæ¬¡æ•°: $(config.n_trials)")
    
    # é¢„ä¼°æ—¶é—´
    est_time_per_trial = device == "GPU" ? 5.0 : 25.0
    est_total_time = est_time_per_trial * config.n_trials
    println("â±ï¸  é¢„ä¼°æ€»æ—¶é—´: $(round(est_total_time/60, digits=1)) åˆ†é’Ÿ")
    
    # è®¾ç½®éšæœºç§å­
    Random.seed!(config.random_seed)
    
    # å‚æ•°ç©ºé—´å®šä¹‰ (é’ˆå¯¹ P4 ä¼˜åŒ–)
    depth_range = 4:10
    lr_range = (0.01, 0.15)
    l2_range = (0.5, 20.0)
    iters_range = 800:200:3000
    
    # å¼€å§‹æœç´¢
    results = Vector{NamedTuple}()
    best_f1 = 0.0
    best_params = nothing
    best_model = nothing
    
    println("\n" * "-"^70)
    println("å¼€å§‹éšæœºæœç´¢...")
    println("-"^70)
    
    for i in 1:config.n_trials
        # éšæœºé‡‡æ ·å‚æ•°
        params = (
            depth = rand(depth_range),
            lr = rand() * (lr_range[2] - lr_range[1]) + lr_range[1],
            l2 = exp(rand() * (log(l2_range[2]) - log(l2_range[1])) + log(l2_range[1])),  # log å‡åŒ€åˆ†å¸ƒ
            iters = rand(iters_range)
        )
        
        result = train_and_evaluate(params, data, device_info, config)
        
        push!(results, (
            params=params,
            f1=result.f1,
            accuracy=result.accuracy,
            time=result.time,
            best_iter=result.best_iter
        ))
        
        # æ›´æ–°æœ€ä½³
        status = ""
        if result.f1 > best_f1
            best_f1 = result.f1
            best_params = params
            best_model = result.model
            status = " â­ NEW BEST!"
        end
        
        # æ‰“å°è¿›åº¦
        println(@sprintf("[%2d/%2d] depth=%d, lr=%.3f, l2=%.2f, iters=%d | F1=%.4f, Acc=%.4f, Time=%.1fs%s",
            i, config.n_trials,
            params.depth, params.lr, params.l2, params.iters,
            result.f1, result.accuracy, result.time, status
        ))
    end
    
    # æ‰“å°æ€»ç»“
    println("\n" * "="^70)
    println("ğŸ“Š éšæœºæœç´¢å®Œæˆ!")
    println("="^70)
    println("æœ€ä½³ F1: $(round(best_f1, digits=4))")
    println("æœ€ä½³å‚æ•°: depth=$(best_params.depth), lr=$(round(best_params.lr, digits=4)), l2=$(round(best_params.l2, digits=2)), iters=$(best_params.iters)")
    println("æ€»ç”¨æ—¶: $(round(sum(r.time for r in results)/60, digits=2)) åˆ†é’Ÿ")
    
    # æ‰“å° Top 5
    sorted_results = sort(results, by=r -> r.f1, rev=true)
    println("\nğŸ† Top 5 é…ç½®:")
    for (i, r) in enumerate(sorted_results[1:min(5, length(sorted_results))])
        println("  #$i: F1=$(round(r.f1, digits=4)), depth=$(r.params.depth), lr=$(round(r.params.lr, digits=3)), l2=$(round(r.params.l2, digits=2))")
    end
    
    return (
        best_f1=best_f1,
        best_params=best_params,
        best_model=best_model,
        all_results=results,
        data=data,
        device=device
    )
end

"""
    save_best_model(result, filename::String)

ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ–‡ä»¶ã€‚
"""
function save_best_model(result, filename::String)
    if result.best_model === nothing
        error("æ²¡æœ‰æœ€ä½³æ¨¡å‹å¯ä¿å­˜ï¼")
    end
    
    result.best_model.save_model(filename)
    println("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: $filename")
end

# ============================================================================
# Hyperopt.jl é›†æˆ - é«˜çº§è¶…å‚æ•°ä¼˜åŒ–
# ============================================================================

"""
    run_catboost_hyperopt(; config=TunerConfig(), sampler=:random)

ä½¿ç”¨ Hyperopt.jl è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚

# Arguments
- `config`: TunerConfig é…ç½®
- `sampler`: é‡‡æ ·å™¨ç±»å‹
  - `:random` - RandomSampler (é»˜è®¤ï¼Œçµæ´»)
  - `:lhs` - LHSampler (æ‹‰ä¸è¶…ç«‹æ–¹ï¼Œæ›´å‡åŒ€è¦†ç›–)
  - `:gp` - GPSampler (é«˜æ–¯è¿‡ç¨‹ï¼Œè´å¶æ–¯ä¼˜åŒ–)
  - `:bluenoise` - BlueNoiseSampler (è“å™ªå£°é‡‡æ ·)

# Returns
- åŒ…å«æœ€ä½³å‚æ•°å’Œä¼˜åŒ–å†å²çš„ NamedTuple

# Tesla P4 ä¼˜åŒ–è¯´æ˜
ç”±äº GPU è®­ç»ƒæå¿« (~5ç§’)ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤šè¿­ä»£æ¬¡æ•°è¿›è¡Œæ›´å½»åº•çš„æœç´¢ã€‚
è´å¶æ–¯ä¼˜åŒ– (GPSampler) åœ¨å‚æ•°ç©ºé—´æ¢ç´¢ä¸Šæ›´é«˜æ•ˆï¼Œæ¨èç”¨äºç²¾ç»†è°ƒä¼˜ã€‚
"""
function run_catboost_hyperopt(; config::TunerConfig=TunerConfig(), sampler::Symbol=:random)
    println("\n" * "="^70)
    println("ğŸ”¬ CatBoost Hyperopt ä¼˜åŒ– (Tesla P4)")
    println("="^70)
    
    # å‡†å¤‡æ•°æ®
    data = prepare_data(config)
    
    # é€‰æ‹©è®¾å¤‡
    device_info = select_device(data.n_samples, config)
    device, border_count = device_info
    println("\nğŸ“± è®¾å¤‡: $device (border_count=$border_count)")
    
    # é€‰æ‹©é‡‡æ ·å™¨
    sampler_obj = if sampler == :random
        println("ğŸ“Š é‡‡æ ·å™¨: RandomSampler")
        RandomSampler()
    elseif sampler == :lhs
        println("ğŸ“Š é‡‡æ ·å™¨: LHSampler (æ‹‰ä¸è¶…ç«‹æ–¹)")
        # LHS éœ€è¦è¿­ä»£æ¬¡æ•°ç­‰äºå€™é€‰æ•°
        RandomSampler()  # å›é€€åˆ° Randomï¼Œå› ä¸º LHS å¯¹ç¦»æ•£å‚æ•°æ”¯æŒæœ‰é™
    elseif sampler == :bohb || sampler == :gp || sampler == :bayesian
        println("ğŸ“Š é‡‡æ ·å™¨: BOHB (è´å¶æ–¯ä¼˜åŒ–)")
        # BOHB éœ€è¦æŒ‡å®šå‚æ•°ç»´åº¦
        BOHB(dims=[
            Categorical(7),   # depth: 7 ä¸ªç¦»æ•£å€¼
            Continuous(),     # lr: è¿ç»­
            Continuous(),     # l2: è¿ç»­
            Categorical(8)    # iters: 8 ä¸ªç¦»æ•£å€¼
        ])
    elseif sampler == :bluenoise
        println("ğŸ“Š é‡‡æ ·å™¨: RandomSampler (BlueNoise ä¸å¯ç”¨)")
        RandomSampler()
    else
        println("ğŸ“Š é‡‡æ ·å™¨: RandomSampler (é»˜è®¤)")
        RandomSampler()
    end
    
    println("ğŸ“Š è¯•éªŒæ¬¡æ•°: $(config.n_trials)")
    
    # é¢„ä¼°æ—¶é—´
    est_time_per_trial = device == "GPU" ? 5.0 : 25.0
    est_total_time = est_time_per_trial * config.n_trials
    println("â±ï¸  é¢„ä¼°æ€»æ—¶é—´: $(round(est_total_time/60, digits=1)) åˆ†é’Ÿ")
    
    # å®šä¹‰æœç´¢ç©ºé—´ (é’ˆå¯¹ P4 ä¼˜åŒ–)
    # depth: 4-10, P4 å¯¹æ·±åº¦ 6-8 æ•ˆç‡æœ€é«˜
    # lr: 0.01-0.15, é…åˆ early_stopping
    # l2: 0.5-20.0, log å‡åŒ€åˆ†å¸ƒ
    # iters: 800-3000
    
    println("\n" * "-"^70)
    println("å¼€å§‹ Hyperopt æœç´¢...")
    println("-"^70)
    
    best_f1 = 0.0
    best_params = nothing
    best_model = nothing
    all_results = Vector{NamedTuple}()
    
    # ä½¿ç”¨ Hyperopt å®
    ho = @hyperopt for i = config.n_trials, 
            sampler = sampler_obj,
            depth = [4, 5, 6, 7, 8, 9, 10],
            lr = LinRange(0.01, 0.15, 50),
            l2 = exp10.(LinRange(-0.3, 1.3, 50)),  # 0.5 ~ 20
            iters = [800, 1000, 1200, 1500, 1800, 2000, 2500, 3000]
        
        # æ„å»ºå‚æ•°
        params = (depth=depth, lr=lr, l2=l2, iters=iters)
        
        # è®­ç»ƒå’Œè¯„ä¼°
        result = train_and_evaluate(params, data, device_info, config)
        
        # è®°å½•ç»“æœ
        push!(all_results, (
            params=params,
            f1=result.f1,
            accuracy=result.accuracy,
            time=result.time,
            best_iter=result.best_iter
        ))
        
        # æ›´æ–°æœ€ä½³
        status = ""
        if result.f1 > best_f1
            best_f1 = result.f1
            best_params = params
            best_model = result.model
            status = " â­ NEW BEST!"
        end
        
        # æ‰“å°è¿›åº¦
        println(@sprintf("[%2d/%2d] depth=%d, lr=%.3f, l2=%.2f, iters=%d | F1=%.4f, Acc=%.4f, Time=%.1fs%s",
            i, config.n_trials,
            depth, lr, l2, iters,
            result.f1, result.accuracy, result.time, status
        ))
        
        # Hyperopt æœ€å°åŒ–ï¼Œæ‰€ä»¥è¿”å›è´Ÿ F1
        -result.f1
    end
    
    # æ‰“å°æ€»ç»“
    println("\n" * "="^70)
    println("ğŸ“Š Hyperopt æœç´¢å®Œæˆ!")
    println("="^70)
    println("æœ€ä½³ F1: $(round(best_f1, digits=4))")
    println("æœ€ä½³å‚æ•°: depth=$(best_params.depth), lr=$(round(best_params.lr, digits=4)), l2=$(round(best_params.l2, digits=2)), iters=$(best_params.iters)")
    println("æ€»ç”¨æ—¶: $(round(sum(r.time for r in all_results)/60, digits=2)) åˆ†é’Ÿ")
    
    # æ‰“å° Top 5
    sorted_results = sort(all_results, by=r -> r.f1, rev=true)
    println("\nğŸ† Top 5 é…ç½®:")
    for (j, r) in enumerate(sorted_results[1:min(5, length(sorted_results))])
        println("  #$j: F1=$(round(r.f1, digits=4)), depth=$(r.params.depth), lr=$(round(r.params.lr, digits=3)), l2=$(round(r.params.l2, digits=2))")
    end
    
    # Hyperopt ç»“æœåˆ†æ
    println("\nğŸ“ˆ Hyperopt åˆ†æ:")
    println("  æœ€ä¼˜å‚æ•°ç»„åˆ: ", ho.minimizer)
    println("  æœ€ä¼˜ç›®æ ‡å€¼ (è´ŸF1): ", round(ho.minimum, digits=4))
    
    return (
        best_f1=best_f1,
        best_params=best_params,
        best_model=best_model,
        all_results=all_results,
        hyperopt_result=ho,
        data=data,
        device=device
    )
end

"""
    run_catboost_bayesian(; config=TunerConfig())

ä¸“é—¨çš„è´å¶æ–¯ä¼˜åŒ–å‡½æ•°ï¼Œä½¿ç”¨ BOHB (Bayesian Optimization Hyperband)ã€‚

BOHB ç»“åˆäº†è´å¶æ–¯ä¼˜åŒ–å’Œæ—©åœç­–ç•¥ï¼Œèƒ½å¤Ÿ:
1. ä½¿ç”¨ KDE (æ ¸å¯†åº¦ä¼°è®¡) å»ºæ¨¡å¥½/åé…ç½®çš„åˆ†å¸ƒ
2. ä¼˜å…ˆé‡‡æ ·æ›´æœ‰å¸Œæœ›çš„é…ç½®
3. è‡ªåŠ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

# Arguments
- `config`: TunerConfig é…ç½®

# Tesla P4 ä¼˜åŒ–
ç”±äº GPU è®­ç»ƒå¿«é€Ÿ (~5ç§’)ï¼ŒBOHB å¯ä»¥å¿«é€Ÿè¿­ä»£æ›´æ–°å…ˆéªŒçŸ¥è¯†ã€‚
"""
function run_catboost_bayesian(; config::TunerConfig=TunerConfig())
    println("\n" * "="^70)
    println("ğŸ§  CatBoost BOHB è´å¶æ–¯ä¼˜åŒ– (Tesla P4)")
    println("="^70)
    
    # å‡†å¤‡æ•°æ®
    data = prepare_data(config)
    
    # é€‰æ‹©è®¾å¤‡
    device_info = select_device(data.n_samples, config)
    device, border_count = device_info
    println("\nğŸ“± è®¾å¤‡: $device (border_count=$border_count)")
    println("ğŸ“Š æ€»è¯•éªŒæ¬¡æ•°: $(config.n_trials)")
    println("ğŸ“Š é‡‡æ ·å™¨: BOHB (Bayesian Optimization Hyperband)")
    
    # é¢„ä¼°æ—¶é—´
    est_time_per_trial = device == "GPU" ? 5.0 : 25.0
    est_total_time = est_time_per_trial * config.n_trials
    println("â±ï¸  é¢„ä¼°æ€»æ—¶é—´: $(round(est_total_time/60, digits=1)) åˆ†é’Ÿ")
    
    println("\n" * "-"^70)
    println("å¼€å§‹ BOHB è´å¶æ–¯ä¼˜åŒ–...")
    println("-"^70)
    
    best_f1 = 0.0
    best_params = nothing
    best_model = nothing
    all_results = Vector{NamedTuple}()
    
    # å‚æ•°ç©ºé—´
    depth_values = [4, 5, 6, 7, 8, 9, 10]
    iters_values = [800, 1000, 1200, 1500, 1800, 2000, 2500, 3000]
    
    # ä½¿ç”¨ BOHB é‡‡æ ·å™¨
    bohb_sampler = BOHB(dims=[
        Categorical(length(depth_values)),   # depth
        Continuous(),                         # lr
        Continuous(),                         # l2 (log scale)
        Categorical(length(iters_values))    # iters
    ])
    
    ho = @hyperopt for i = config.n_trials,
            sampler = bohb_sampler,
            depth_idx = 1:length(depth_values),
            lr = LinRange(0.01, 0.15, 50),
            l2_log = LinRange(-0.3, 1.3, 50),  # 0.5 ~ 20
            iters_idx = 1:length(iters_values)
        
        # æ˜ å°„å‚æ•°
        depth = depth_values[depth_idx]
        iters = iters_values[iters_idx]
        l2 = 10.0^l2_log
        
        params = (depth=depth, lr=lr, l2=l2, iters=iters)
        result = train_and_evaluate(params, data, device_info, config)
        
        push!(all_results, (
            params=params,
            f1=result.f1,
            accuracy=result.accuracy,
            time=result.time,
            best_iter=result.best_iter
        ))
        
        # æ›´æ–°æœ€ä½³
        status = ""
        if result.f1 > best_f1
            best_f1 = result.f1
            best_params = params
            best_model = result.model
            status = " â­ NEW BEST!"
        end
        
        println(@sprintf("[%2d/%2d] depth=%d, lr=%.3f, l2=%.2f, iters=%d | F1=%.4f, Acc=%.4f, Time=%.1fs%s",
            length(all_results), config.n_trials,
            depth, lr, l2, iters,
            result.f1, result.accuracy, result.time, status
        ))
        
        -result.f1  # æœ€å°åŒ–è´Ÿ F1
    end
    
    # æ‰“å°æ€»ç»“
    println("\n" * "="^70)
    println("ğŸ“Š BOHB è´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
    println("="^70)
    println("æœ€ä½³ F1: $(round(best_f1, digits=4))")
    if best_params !== nothing
        println("æœ€ä½³å‚æ•°: depth=$(best_params.depth), lr=$(round(best_params.lr, digits=4)), l2=$(round(best_params.l2, digits=2)), iters=$(best_params.iters)")
    end
    println("æ€»ç”¨æ—¶: $(round(sum(r.time for r in all_results)/60, digits=2)) åˆ†é’Ÿ")
    
    # æ‰“å° Top 5
    sorted_results = sort(all_results, by=r -> r.f1, rev=true)
    println("\nğŸ† Top 5 é…ç½®:")
    for (j, r) in enumerate(sorted_results[1:min(5, length(sorted_results))])
        println("  #$j: F1=$(round(r.f1, digits=4)), depth=$(r.params.depth), lr=$(round(r.params.lr, digits=3)), l2=$(round(r.params.l2, digits=2))")
    end
    
    return (
        best_f1=best_f1,
        best_params=best_params,
        best_model=best_model,
        all_results=all_results,
        hyperopt_result=ho,
        data=data,
        device=device
    )
end

end # module
