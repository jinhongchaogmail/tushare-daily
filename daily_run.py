import os
import sys
import time
import pandas as pd
import requests
import xcsc_tushare as ts
from datetime import datetime

TUSHARE_TOKEN = os.environ.get("TUSHARE_TOKEN")
TS_SERVER = "http://116.128.206.39:7172"
TS_ENV = "prd"
START_DATE = "20220101"
OUT_DIR = "data"
MODEL_PATH = 'models/catboost_final_model.cbm'
PARAMS_PATH = 'models/final_model_params.json'
MIN_RETURN_THRESHOLD = 0.03

# åŠ¨æ€å¯¼å…¥ç‰¹å¾å·¥ç¨‹é€»è¾‘ (ä¼˜å…ˆä½¿ç”¨æ¨¡å‹ç»‘å®šçš„ frozen_features.py)
# è¿™æ ·å¯ä»¥ä¿è¯é¢„æµ‹æ—¶ä½¿ç”¨çš„ç‰¹å¾è®¡ç®—é€»è¾‘ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼Œ
# å³ä½¿ shared/features.py å·²ç»æ›´æ–°æˆ–ä¿®æ”¹ã€‚
HAS_FEATURE_ENGINE = False
apply_technical_indicators = None

def load_feature_engineering():
    global apply_technical_indicators, HAS_FEATURE_ENGINE
    
    # 1. å°è¯•åŠ è½½æ¨¡å‹ç›®å½•ä¸‹çš„ frozen_features.py (æ¨¡å‹ä¼´ç”Ÿä»£ç )
    frozen_features_path = os.path.join(os.path.dirname(MODEL_PATH), 'frozen_features.py')
    if os.path.exists(frozen_features_path):
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("frozen_features", frozen_features_path)
            module = importlib.util.module_from_spec(spec)
            sys.modules["frozen_features"] = module
            spec.loader.exec_module(module)
            apply_technical_indicators = module.apply_technical_indicators
            HAS_FEATURE_ENGINE = True
            print(f"âœ… å·²åŠ è½½æ¨¡å‹ä¼´ç”Ÿç‰¹å¾ä»£ç : {frozen_features_path}", flush=True)
            return
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ frozen_features.py å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ° shared/features.py", flush=True)

    # 2. å›é€€åˆ°é¡¹ç›®é»˜è®¤çš„ shared/features.py
    try:
        sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'shared'))
        from features import apply_technical_indicators as shared_ati
        apply_technical_indicators = shared_ati
        HAS_FEATURE_ENGINE = True
        print("âœ… å·²åŠ è½½é»˜è®¤ç‰¹å¾ä»£ç : shared/features.py", flush=True)
    except ImportError as e:
        print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç‰¹å¾å·¥ç¨‹ ({e})ï¼Œå°†è·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
        HAS_FEATURE_ENGINE = False

load_feature_engineering()

# å°è¯•å¯¼å…¥ CatBoost
try:
    import catboost as cb
    import json
    HAS_MODEL = True
except ImportError:
    print("âš ï¸ è­¦å‘Šï¼šæœªå®‰è£… catboostï¼Œå°†è·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
    HAS_MODEL = False

# å…¨å±€å˜é‡
model = None
vol_multiplier = 0.89
report = []
all_predictions = [] # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœï¼Œç”¨äºå¼ºåˆ¶è¾“å‡º
count_debug = 0

if not TUSHARE_TOKEN:
    raise RuntimeError("Missing env TUSHARE_TOKEN")

ts.set_token(TUSHARE_TOKEN)
pro = ts.pro_api(env=TS_ENV, server=TS_SERVER)

# 1. åŸºç¡€è¡Œæƒ…å­—æ®µ (åŒ…å«äº¤æ˜“çŠ¶æ€)
fields_daily = "ts_code,trade_date,open,high,low,close,pre_close,change,pct_chg,volume,amount,adj_factor,trade_status"
# 2. æ¯æ—¥æŒ‡æ ‡å­—æ®µ (æ³¨æ„ XCSC ç‰¹æœ‰å­—æ®µå: tot_mv, turn)
fields_daily_basic = "ts_code,trade_date,tot_mv,mv,turn,pe,pe_ttm,pb_new,free_turnover,high_52w,low_52w"
# 3. èµ„é‡‘æµå‘å­—æ®µ
fields_moneyflow = "ts_code,trade_date,buy_sm_vol,sell_sm_vol,buy_md_vol,sell_md_vol,buy_lg_vol,sell_lg_vol,buy_elg_vol,sell_elg_vol,net_mf_vol,net_mf_amount"

def init_model():
    """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
    global model, vol_multiplier
    
    if not HAS_MODEL or not HAS_FEATURE_ENGINE:
        return False
    
    if not os.path.exists(MODEL_PATH):
        print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}ï¼Œè·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
        return False

    try:
        model = cb.CatBoostClassifier()
        model.load_model(MODEL_PATH)
        
        if os.path.exists(PARAMS_PATH):
            with open(PARAMS_PATH, 'r') as f:
                params = json.load(f)
                vol_multiplier = params.get('vol_multiplier_best', 0.89)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ³¢åŠ¨ç‡ä¹˜æ•°: {vol_multiplier:.4f}", flush=True)
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", flush=True)
        return False

def predict_stock(ts_code, df):
    """å¯¹å•åªè‚¡ç¥¨è¿›è¡Œé¢„æµ‹ (å¤šç©ºåŒå‘)"""
    global report, count_debug, all_predictions
    
    if model is None or len(df) < 60:
        if len(df) < 60:
            print(f"  [{ts_code}] æ•°æ®ä¸è¶³ ({len(df)}è¡Œ)ï¼Œè·³è¿‡é¢„æµ‹", flush=True)
        return

    try:
        # ç‰¹å¾å·¥ç¨‹
        # 1. åŸºç¡€ç‰¹å¾ (shared)
        # (v32: shared/features.py ç°åœ¨åŒ…å«æ‰€æœ‰ç‰¹å¾é€»è¾‘ï¼ŒåŒ…æ‹¬ close_lag1, volume_change ç­‰)
        df_features = apply_technical_indicators(df)
        
        # 2. (v32: ç§»é™¤æ‰‹åŠ¨è¡¥å…¨ï¼Œå·²ç»Ÿä¸€åˆ° shared/features.py)
        # if 'close_lag1' not in df_features.columns: ...
        
        latest_row = df_features.iloc[[-1]].copy()
        current_date = latest_row['trade_date'].values[0]
        
        # é¢„æµ‹
        # v28 ä¿®å¤: CatBoost é¢„æµ‹æ—¶ç‰¹å¾é¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
        # è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº (å‚è€ƒ training/train.py get_feature_columns)
        # è¿™é‡Œæˆ‘ä»¬åŠ¨æ€è·å–æ¨¡å‹éœ€è¦çš„ç‰¹å¾å
        model_feature_names = model.feature_names_
        
        # æ£€æŸ¥ç¼ºå¤±ç‰¹å¾
        missing_features = [f for f in model_feature_names if f not in latest_row.columns]
        if missing_features:
            print(f"  [{ts_code}] ç¼ºå¤±ç‰¹å¾: {missing_features}ï¼Œè·³è¿‡", flush=True)
            return

        # æŒ‰æ¨¡å‹è¦æ±‚çš„é¡ºåºé‡æ’ç‰¹å¾
        X_predict = latest_row[model_feature_names]
        
        prob = model.predict_proba(X_predict)[0]
        prob_down, prob_flat, prob_up = prob[0], prob[1], prob[2]
        
        # è°ƒè¯•è¾“å‡º
        count_debug += 1
        # å¦‚æœæ€»æ•°å°‘äº 200 (è°ƒè¯•æ¨¡å¼)ï¼Œåˆ™æ‰“å°æ‰€æœ‰é¢„æµ‹ç»“æœ
        if len(report) < 200 or count_debug <= 5 or count_debug % 200 == 0 or prob_up > 0.25:
            print(f"  [{ts_code}] é¢„æµ‹: è·Œ{prob_down:.2f} å¹³{prob_flat:.2f} æ¶¨{prob_up:.2f}", flush=True)

        # ç­–ç•¥é€»è¾‘
        current_vol = latest_row['volatility_factor'].values[0]
        if pd.isna(current_vol): 
            current_vol = 0.02
        
        implied_return = current_vol * vol_multiplier
        signal = "âšª è§‚æœ›"
        position = 0.0
        reason = ""
        is_candidate = False
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ (Debugç”¨)
        all_predictions.append({
            'ä»£ç ': ts_code,
            'æ—¥æœŸ': pd.to_datetime(current_date).strftime('%Y-%m-%d'),
            'ä¿¡å·': "Debug",
            'ä¸Šæ¶¨æ¦‚ç‡': f"{prob_up:.1%}",
            'ä¸‹è·Œæ¦‚ç‡': f"{prob_down:.1%}",
            'æ³¢åŠ¨ç‡': f"{current_vol:.1%}",
            'é¢„æœŸæ”¶ç›Š': f"{implied_return:.1%}",
            'å»ºè®®ä»“ä½': "0.0%",
            'ç†ç”±': "Debugè®°å½•",
            'prob_up_raw': prob_up,
            'prob_down_raw': prob_down,
            'max_prob': max(prob_up, prob_down)
        })
        
        # --- é˜ˆå€¼è®¾ç½® (åŸºäºæœ€æ–°æ¨¡å‹åˆ†æ: Down å‡†, Up ä¿å®ˆ) ---
        THRESHOLD_WATCH_UP = 0.28    # é™ä½åšå¤šé—¨æ§›
        THRESHOLD_STRONG_UP = 0.38   # å¼ºåŠ›åšå¤šé—¨æ§›
        THRESHOLD_WATCH_DOWN = 0.35  # åšç©ºé—¨æ§›
        THRESHOLD_STRONG_DOWN = 0.45 # å¼ºåŠ›åšç©ºé—¨æ§›

        # --- 1. åšå¤šä¿¡å· (Long) ---
        if prob_up > prob_down and prob_up > THRESHOLD_WATCH_UP:
            signal = "ğŸ”µ å…³æ³¨å¤š"
            reason = f"çœ‹æ¶¨({prob_up:.1%})"
            is_candidate = True
            
            # å¼ºåŠ›ä¹°å…¥æ¡ä»¶
            if prob_up > THRESHOLD_STRONG_UP and prob_up > prob_flat:
                if implied_return > MIN_RETURN_THRESHOLD:
                    signal = "ğŸ”´ å¼ºåŠ›åšå¤š"
                    position = min(1.0, 0.02 / (current_vol + 1e-5))
                    reason = f"é«˜èƒœç‡({prob_up:.0%}) é«˜èµ”ç‡(>{implied_return:.1%})"
                else:
                    signal = "ğŸŸ  æ½œä¼åšå¤š" # èƒœç‡é«˜ä½†æ³¢åŠ¨ç‡ä½
                    reason = f"é«˜èƒœç‡({prob_up:.0%}) ä½æ³¢åŠ¨"

        # --- 2. åšç©ºä¿¡å· (Short) ---
        elif prob_down > prob_up and prob_down > THRESHOLD_WATCH_DOWN:
            signal = "ğŸŸ¡ å…³æ³¨ç©º"
            reason = f"çœ‹è·Œ({prob_down:.1%})"
            is_candidate = True
            
            if prob_down > THRESHOLD_STRONG_DOWN and prob_down > prob_flat:
                signal = "ğŸŸ¢ å¼ºåŠ›åšç©º"
                reason = f"é«˜ç¡®ä¿¡åº¦({prob_down:.1%})"
                position = min(1.0, 0.02 / (current_vol + 1e-5))

        if is_candidate:
            item = {
                'ä»£ç ': ts_code,
                'æ—¥æœŸ': pd.to_datetime(current_date).strftime('%Y-%m-%d'),
                'ä¿¡å·': signal,
                'ä¸Šæ¶¨æ¦‚ç‡': f"{prob_up:.1%}",
                'ä¸‹è·Œæ¦‚ç‡': f"{prob_down:.1%}",
                'æ³¢åŠ¨ç‡': f"{current_vol:.1%}",
                'é¢„æœŸæ”¶ç›Š': f"{implied_return:.1%}",
                'å»ºè®®ä»“ä½': f"{position:.1%}",
                'ç†ç”±': reason,
                'prob_up_raw': prob_up,
                'prob_down_raw': prob_down,
                'max_prob': max(prob_up, prob_down)
            }
            report.append(item)
            if "å¼ºåŠ›" in signal:
                print(f"  !!! å‘ç°æœºä¼š [{ts_code}]: {signal} - {reason}", flush=True)

    except Exception as e:
        print(f"âŒ [{ts_code}] é¢„æµ‹å‡ºé”™: {e}", flush=True)
        import traceback
        traceback.print_exc()

def generate_report():
    """ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š (åˆ†å¤šç©ºå±•ç¤º)"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    report_path = "reports/strategy_report.md"
    
    if report:
        df_report = pd.DataFrame(report)
        
        # åˆ†ç¦»å¤šç©º
        df_long = df_report[df_report['ä¿¡å·'].str.contains('å¤š')].sort_values('prob_up_raw', ascending=False)
        df_short = df_report[df_report['ä¿¡å·'].str.contains('ç©º')].sort_values('prob_down_raw', ascending=False)
        
        # ç§»é™¤åŸå§‹æ’åºåˆ—ï¼Œä¿æŒè¡¨æ ¼æ•´æ´
        cols_to_drop = ['prob_up_raw', 'prob_down_raw', 'max_prob']
        df_long_display = df_long.drop(columns=cols_to_drop, errors='ignore')
        df_short_display = df_short.drop(columns=cols_to_drop, errors='ignore')
        
        print(f"\n=== æ¯æ—¥ç­–ç•¥æŠ¥å‘Š (æ€»è®¡: {len(df_report)}) ===", flush=True)
        print(f"å¤šå¤´æœºä¼š: {len(df_long)} | ç©ºå¤´æœºä¼š: {len(df_short)}", flush=True)
        
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        with open(report_path, "w") as f:
            f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str})\n\n")
            f.write(f"**æ€»è®¡å…¥é€‰**: {len(df_report)} (å¤šå¤´: {len(df_long)}, ç©ºå¤´: {len(df_short)})\n\n")
            
            f.write("## ğŸ”´ å¤šå¤´æœºä¼š (Top 50)\n")
            if not df_long.empty:
                f.write(df_long_display.head(50).to_markdown(index=False))
            else:
                f.write("æ— ç¬¦åˆæ¡ä»¶çš„å¤šå¤´æœºä¼šã€‚\n")
            
            f.write("\n\n## ğŸŸ¢ ç©ºå¤´æœºä¼š (Top 50)\n")
            if not df_short.empty:
                f.write(df_short_display.head(50).to_markdown(index=False))
            else:
                f.write("æ— ç¬¦åˆæ¡ä»¶çš„ç©ºå¤´æœºä¼šã€‚\n")
        
        csv_path = report_path.replace(".md", ".csv")
        # CSV ä¿ç•™åŸå§‹æ¦‚ç‡åˆ—ï¼Œæ–¹ä¾¿ç”¨æˆ·è‡ªå·±æ’åº
        df_report.sort_values('max_prob', ascending=False).to_csv(csv_path, index=False)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path} å’Œ {csv_path}", flush=True)
    else:
        print("â„¹ï¸ ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“æœºä¼š", flush=True)
        
        # --- å¼ºåˆ¶è¾“å‡º Top 10 (Debug) ---
        if all_predictions:
            print("âš ï¸ å¼ºåˆ¶è¾“å‡º Top 10 é¢„æµ‹ç»“æœ (å³ä½¿æœªè¾¾é˜ˆå€¼)", flush=True)
            df_all = pd.DataFrame(all_predictions)
            # æŒ‰æœ€å¤§æ¦‚ç‡æ’åº
            df_top = df_all.sort_values('max_prob', ascending=False).head(10)
            
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            with open(report_path, "w") as f:
                f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str}) - DEBUG MODE\n\n")
                f.write("âš ï¸ **æ³¨æ„**: ä»Šæ—¥æ— ç¬¦åˆé˜ˆå€¼çš„æœºä¼šã€‚ä»¥ä¸‹ä¸ºæ¦‚ç‡æœ€é«˜çš„ Top 10 è‚¡ç¥¨ (ä»…ä¾›è°ƒè¯•å‚è€ƒ)ã€‚\n\n")
                f.write(df_top.drop(columns=['prob_up_raw', 'prob_down_raw', 'max_prob'], errors='ignore').to_markdown(index=False))
            
            print(f"âœ… å¼ºåˆ¶æŠ¥å‘Šå·²ä¿å­˜: {report_path}", flush=True)
        else:
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            with open(report_path, "w") as f:
                f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str})\n\n")
                f.write("ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“æœºä¼šï¼Œä¸”æ— ä»»ä½•æœ‰æ•ˆé¢„æµ‹æ•°æ®ã€‚")

def get_hist(ts_code: str):
    """
    è·å–å…¨é‡æ•°æ®ï¼šè¡Œæƒ… + æ¯æ—¥æŒ‡æ ‡ + èµ„é‡‘æµå‘
    å¹¶åˆå¹¶ä¸ºä¸€ä¸ª DataFrame
    """
    try:
        # 1. è·å–æ—¥çº¿è¡Œæƒ…
        df_daily = pro.daily(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_daily)
        if df_daily.empty:
            return None
            
        # 2. è·å–æ¯æ—¥æŒ‡æ ‡ (å¸‚å€¼, æ¢æ‰‹, PE/PB)
        # æ³¨æ„: æ¥å£å¯èƒ½è¿”å›ç©ºï¼Œéœ€å¤„ç†
        try:
            df_basic = pro.daily_basic(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_daily_basic)
        except Exception:
            df_basic = pd.DataFrame()
            
        # 3. è·å–èµ„é‡‘æµå‘
        try:
            # moneyflow æœ‰æ—¶åœ¨ç½‘ç»œæˆ–æœåŠ¡ç«¯è¾ƒæ…¢ï¼Œè®¾ç½®çŸ­è¶…æ—¶ä¿æŠ¤
            # xcsc_tushare çš„ pro.moneyflow æœ¬èº«æ—  timeout å‚æ•°ï¼Œå› æ­¤ä½¿ç”¨çº¿ç¨‹åŒ…è£…ä»¥é¿å…é˜»å¡
            import concurrent.futures

            def call_moneyflow():
                return pro.moneyflow(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_moneyflow)

            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(call_moneyflow)
                try:
                    df_flow = future.result(timeout=10)  # 10ç§’è¶…æ—¶
                except Exception:
                    future.cancel()
                    df_flow = pd.DataFrame()
        except Exception:
            df_flow = pd.DataFrame()

        # --- æ•°æ®åˆå¹¶ ---
        # ä»¥ daily ä¸ºä¸»è¡¨ï¼Œå·¦è¿æ¥å…¶ä»–è¡¨
        df_merge = df_daily
        
        if not df_basic.empty:
            df_merge = pd.merge(df_merge, df_basic, on=['ts_code', 'trade_date'], how='left')
            
        if not df_flow.empty:
            df_merge = pd.merge(df_merge, df_flow, on=['ts_code', 'trade_date'], how='left')

        # ç»Ÿä¸€æŒ‰æ—¥æœŸæ’åº (æ—§åˆ°æ–°)
        df_merge = df_merge.sort_values('trade_date').reset_index(drop=True)
        # ç¡®ä¿æ—¥æœŸæ ¼å¼
        df_merge["trade_date"] = pd.to_datetime(df_merge["trade_date"], format="%Y%m%d")

        # åœ¨è¿‡æ»¤åœç‰Œä¹‹å‰ï¼Œå°è¯•æ‹‰å–å¹¶åˆå¹¶å­£æŠ¥è´¢åŠ¡æ•°æ®ï¼ˆå®‰å…¨å¯¹é½ï¼‰
        try:
            from shared.financials import fetch_financials, align_financials_to_daily
            df_fin = fetch_financials(pro, ts_code, start_date=START_DATE)
            if df_fin is not None and not df_fin.empty:
                aligned_fin = align_financials_to_daily(df_merge, df_fin)
                # åˆå¹¶åˆ°ä¸»è¡¨ï¼ˆæŒ‰è¡Œå¯¹é½ï¼‰ï¼Œä¿ç•™è´¢åŠ¡å­—æ®µ
                try:
                    df_merge = pd.concat([df_merge.reset_index(drop=True), aligned_fin.reset_index(drop=True)], axis=1)
                except Exception:
                    pass
        except Exception:
            # è´¢åŠ¡æŠ“å–éå…³é”®ï¼Œå¤±è´¥æ—¶ç»§ç»­
            pass

        # è¿‡æ»¤åœç‰Œæ—¥å¹¶å¯¹åˆšå¤ç‰Œçš„è‚¡ç¥¨åšçŸ­æœŸä¿æŠ¤ï¼ˆé˜²æ­¢å¤ç‰Œå¼‚å¸¸æ³¢åŠ¨æ±¡æŸ“ç‰¹å¾ï¼‰
        RESUME_SAFE_DAYS = 5
        if 'trade_status' in df_merge.columns:
            # æ ‡è®°æ˜¯å¦äº¤æ˜“ï¼šXCSC ä½¿ç”¨ä¸­æ–‡å­—æ®µå€¼ï¼ˆä¾‹å¦‚ 'äº¤æ˜“'ã€'åœç‰Œ'ã€'XD' ç­‰ï¼‰
            # å…¼å®¹å¸¸è§å€¼ï¼š'T','äº¤æ˜“','TRADE','äº¤æ˜“ä¸­','1'
            valid_trade_vals = set(['T', 'äº¤æ˜“', 'TRADE', 'äº¤æ˜“ä¸­', '1'])
            # æœ‰äº›å€¼å¯èƒ½åŒ…å«ç©ºæ ¼æˆ–å¤§å°å†™å·®å¼‚ï¼Œç»Ÿä¸€å¤„ç†
            def is_trade_val(x):
                try:
                    s = str(x).strip()
                except Exception:
                    return False
                return s in valid_trade_vals or s == 'äº¤æ˜“'
            df_merge['is_trade'] = df_merge['trade_status'].apply(is_trade_val)
            # å°†è¿ç»­æ®µåˆ†ç»„
            grp = (df_merge['is_trade'] != df_merge['is_trade'].shift(fill_value=df_merge['is_trade'].iloc[0])).cumsum()
            df_merge['grp'] = grp
            df_merge['days_since_resume'] = 0
            grp_vals = df_merge.groupby('grp')['is_trade'].first().to_dict()
            groups = sorted(grp_vals.items(), key=lambda x: x[0])
            # iterate groups to find resume groups (group with is_trade==True and previous group is_trade==False)
            for idx in range(1, len(groups)):
                gid, val = groups[idx]
                prev_gid, prev_val = groups[idx-1]
                if val and not prev_val:
                    # this group is a resume after suspension
                    mask = df_merge['grp'] == gid
                    n = mask.sum()
                    # set 1..n
                    df_merge.loc[mask, 'days_since_resume'] = list(range(1, n+1))
            # åˆ é™¤éäº¤æ˜“æ—¥
            df_merge = df_merge[df_merge['is_trade']]
            # åˆ é™¤å¤ç‰ŒåçŸ­æœŸæ•°æ®
            df_merge = df_merge[~((df_merge['days_since_resume'] > 0) & (df_merge['days_since_resume'] <= RESUME_SAFE_DAYS))]
            # æ¸…ç†è¾…åŠ©åˆ—
            df_merge.drop(columns=['is_trade', 'grp', 'days_since_resume'], inplace=True, errors='ignore')

        # --- å•ä½æ ¡æ­£: æ¨æ–­å¹¶ç»Ÿä¸€é‡/é¢å•ä½åˆ°â€œè‚¡â€å’Œäººæ°‘å¸é‡‘é¢ ---
        try:
            if 'volume' in df_merge.columns and 'amount' in df_merge.columns and 'close' in df_merge.columns:
                mask = df_merge['volume'].notna() & df_merge['amount'].notna() & df_merge['close'].notna() & (df_merge['close']>0) & (df_merge['volume']>0)
                if mask.sum() >= 5:
                    ratios = (df_merge.loc[mask, 'amount'] / (df_merge.loc[mask, 'volume'] * df_merge.loc[mask, 'close'] + 1e-12)).replace([float('inf'), -float('inf')], pd.NA).dropna()
                    if len(ratios) >= 3:
                        scale = float(ratios.median())
                        if 1e-6 < scale < 1e6:
                            # æ ‡å‡†åŒ–åˆ—
                            df_merge['volume_shares'] = df_merge['volume'] * scale
                            df_merge['amount_cny'] = df_merge['volume_shares'] * df_merge['close']
                            df_merge['volume_scale_inferred'] = scale
                            # moneyflow é‡‘é¢åˆ—è°ƒæ•´
                            if 'net_mf_amount' in df_merge.columns and df_merge['net_mf_amount'].notna().sum() > 0:
                                orig_med = df_merge.loc[mask, 'amount'].median()
                                new_med = df_merge.loc[mask, 'amount_cny'].median()
                                if orig_med and abs(orig_med) > 0:
                                    amt_factor = new_med / orig_med
                                    df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount'] * amt_factor
                                else:
                                    df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount']
        except Exception:
            # å•ä¸ªç¥¨çš„å½’ä¸€åŒ–å¤±è´¥ä¸åº”ä¸­æ–­æ•´ä¸ªæµç¨‹
            pass

        # ä»…è¿›è¡Œé™ç²¾åº¦å¤„ç†ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–ç‰¹å¾ï¼Œä¿æŒæ•°æ®çº¯æ´
        df_merge = downcast(df_merge)

        if len(df_merge) > 21:
            return ts_code, df_merge
        else:
            return None

    except Exception as e:
        raise e

def list_main_board_cs():
    """è·å–ä¸»æ¿å·²ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨"""
    today = datetime.today().strftime("%Y%m%d")
    temp0 = pro.stock_basic(market="CS", fields="ts_code,name,list_date,delist_date,list_board_name")
    temp0 = temp0[temp0["delist_date"].isna()]  # æœªé€€å¸‚
    temp0 = temp0[temp0["list_board_name"] == "ä¸»æ¿"]  # ä¸»æ¿
    temp0 = temp0[temp0["list_date"] <= today]  # å·²ç»ä¸Šå¸‚
    return temp0[["ts_code", "name"]].reset_index(drop=True)

# (v32: å·²ç§»é™¤ add_features å‡½æ•°ï¼Œç¡®ä¿ä¿å­˜çš„æ•°æ®åªåŒ…å«åŸå§‹è¡Œæƒ…)

def downcast(df: pd.DataFrame) -> pd.DataFrame:
    """é™ä½æµ®ç‚¹ç²¾åº¦ä»¥èŠ‚çœç©ºé—´"""
    for col in df.select_dtypes(include=["float64"]).columns:
        df[col] = df[col].astype("float32")
    return df

def main():
    print("ğŸš€ å¯åŠ¨æ•°æ®ä¸‹è½½ä¸é¢„æµ‹è„šæœ¬ (å•çº¿ç¨‹æ¨¡å¼)...", flush=True)
    os.makedirs(OUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    model_enabled = init_model()
    if model_enabled:
        print("ğŸ“Š é¢„æµ‹åŠŸèƒ½å·²å¯ç”¨", flush=True)
    else:
        print("ğŸ“Š é¢„æµ‹åŠŸèƒ½æœªå¯ç”¨ï¼Œä»…ä¸‹è½½æ•°æ®", flush=True)
    
    print("ğŸ“‹ æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...", flush=True)
    try:
        ts_codes = list_main_board_cs()
    except Exception as e:
        print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}", flush=True)
        return

    print(f"âœ… è·å–åˆ° {len(ts_codes)} åªè‚¡ç¥¨ï¼Œå¼€å§‹å¤„ç†...", flush=True)
    
    # å…¨é‡ä¸‹è½½æ¨¡å¼ï¼šé»˜è®¤å¤„ç†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨
    print("âš ï¸ å…¨é‡ä¸‹è½½æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨", flush=True)
    
    total = len(ts_codes)
    skipped = []
    
    for idx, row in ts_codes.iterrows():
        ts_code = row["ts_code"]
        
        # è¿›åº¦æ‰“å°
        if idx % 50 == 0:
            print(f"[{idx}/{total}] æ­£åœ¨å¤„ç†: {ts_code}", flush=True)
            
        i = None
        retry = 0
        max_retry = 3

        while i is None and retry < max_retry:
            try:
                i = get_hist(ts_code)
                if i is None:
                    skipped.append(ts_code)
                    break
            except requests.exceptions.ConnectionError:
                print(f"âš ï¸ {ts_code} ç½‘ç»œé”™è¯¯ï¼Œ3ç§’åé‡è¯•", flush=True)
                time.sleep(3)
                retry += 1
                continue
            except Exception as e:
                print(f"âŒ {ts_code} å‡ºé”™: {e}ï¼Œè·³è¿‡", flush=True)
                skipped.append(ts_code)
                break

        if i is not None:
            code, df = i
            try:
                # é¢„æµ‹ï¼ˆåœ¨ä¿å­˜å‰ï¼‰
                if model_enabled and model is not None:
                    # ä½¿ç”¨å‰¯æœ¬è¿›è¡Œé¢„æµ‹ï¼Œä¸æ±¡æŸ“åŸå§‹æ•°æ®
                    predict_stock(code, df.copy())

                # ä¿å­˜ï¼ˆget_hist å·²ç»å®Œæˆå½’ä¸€åŒ–ä¸ downcastï¼‰
                out_file = os.path.join(OUT_DIR, f"{code}.parquet")
                df.to_parquet(out_file, engine="pyarrow", compression="zstd", compression_level=3, index=False)
            except Exception as e:
                print(f"âŒ {ts_code} å¤„ç†æ•°æ®å‡ºé”™: {e}", flush=True)
                skipped.append(ts_code)

    if skipped:
        pd.DataFrame(skipped, columns=["ts_code"]).to_csv("skipped.csv", index=False)
        print(f"âš ï¸ è·³è¿‡ {len(skipped)} ä¸ªè‚¡ç¥¨ï¼Œå·²å†™å…¥ skipped.csv", flush=True)

    # ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
    if model_enabled and model is not None:
        generate_report()

    print("ğŸ‰ RUN_DONE: æ‰€æœ‰ä»»åŠ¡å®Œæˆ", flush=True)

if __name__ == "__main__":
    main()
