import os
import sys
import time
import pandas as pd
import requests
import xcsc_tushare as ts
from datetime import datetime, timedelta

TUSHARE_TOKEN = os.environ.get("TUSHARE_TOKEN")
TS_SERVER = "http://116.128.206.39:7172"
TS_ENV = "prd"
START_DATE = "20220101"
OUT_DIR = "data"
MODEL_PATH = 'models/catboost_final_model.cbm'
PARAMS_PATH = 'models/final_model_params.json'
MIN_RETURN_THRESHOLD = 0.03

# åŠ¨æ€å¯¼å…¥ç‰¹å¾å·¥ç¨‹é€»è¾‘ (ä¼˜å…ˆä½¿ç”¨æ¨¡å‹ç»‘å®šçš„ frozen_features.py)
# è¿™æ ·å¯ä»¥ä¿è¯é¢„æµ‹æ—¶ä½¿ç”¨çš„ç‰¹å¾è®¡ç®—é€»è¾‘ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼Œ
# å³ä½¿ shared/features.py å·²ç»æ›´æ–°æˆ–ä¿®æ”¹ã€‚
HAS_FEATURE_ENGINE = False
apply_technical_indicators = None

def load_feature_engineering():
    global apply_technical_indicators, HAS_FEATURE_ENGINE
    
    # 1. å°è¯•åŠ è½½æ¨¡å‹ç›®å½•ä¸‹çš„ frozen_features.py (æ¨¡å‹ä¼´ç”Ÿä»£ç )
    frozen_features_path = os.path.join(os.path.dirname(MODEL_PATH), 'frozen_features.py')
    if os.path.exists(frozen_features_path):
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("frozen_features", frozen_features_path)
            module = importlib.util.module_from_spec(spec)
            sys.modules["frozen_features"] = module
            spec.loader.exec_module(module)
            apply_technical_indicators = module.apply_technical_indicators
            HAS_FEATURE_ENGINE = True
            print(f"âœ… å·²åŠ è½½æ¨¡å‹ä¼´ç”Ÿç‰¹å¾ä»£ç : {frozen_features_path}", flush=True)
            return
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ frozen_features.py å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ° shared/features.py", flush=True)

    # 2. å›é€€åˆ°é¡¹ç›®é»˜è®¤çš„ shared/features.py
    try:
        sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'shared'))
        from features import apply_technical_indicators as shared_ati
        apply_technical_indicators = shared_ati
        HAS_FEATURE_ENGINE = True
        print("âœ… å·²åŠ è½½é»˜è®¤ç‰¹å¾ä»£ç : shared/features.py", flush=True)
    except ImportError as e:
        print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç‰¹å¾å·¥ç¨‹ ({e})ï¼Œå°†è·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
        HAS_FEATURE_ENGINE = False

load_feature_engineering()

# å°è¯•å¯¼å…¥ CatBoost
try:
    import catboost as cb
    import json
    HAS_MODEL = True
except ImportError:
    print("âš ï¸ è­¦å‘Šï¼šæœªå®‰è£… catboostï¼Œå°†è·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
    HAS_MODEL = False

# å…¨å±€å˜é‡
model = None
vol_multiplier = 0.89
report = []
all_predictions = [] # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœï¼Œç”¨äºå¼ºåˆ¶è¾“å‡º
count_debug = 0

if not TUSHARE_TOKEN:
    raise RuntimeError("Missing env TUSHARE_TOKEN")

ts.set_token(TUSHARE_TOKEN)
pro = ts.pro_api(env=TS_ENV, server=TS_SERVER)

# 1. åŸºç¡€è¡Œæƒ…å­—æ®µ (åŒ…å«äº¤æ˜“çŠ¶æ€)
fields_daily = "ts_code,trade_date,open,high,low,close,pre_close,change,pct_chg,volume,amount,adj_factor,trade_status"
# 2. æ¯æ—¥æŒ‡æ ‡å­—æ®µ (æ³¨æ„ XCSC ç‰¹æœ‰å­—æ®µå: tot_mv, turn)
fields_daily_basic = "ts_code,trade_date,tot_mv,mv,turn,pe,pe_ttm,pb_new,free_turnover,high_52w,low_52w"
# 3. èµ„é‡‘æµå‘å­—æ®µ
fields_moneyflow = "ts_code,trade_date,buy_sm_vol,sell_sm_vol,buy_md_vol,sell_md_vol,buy_lg_vol,sell_lg_vol,buy_elg_vol,sell_elg_vol,net_mf_vol,net_mf_amount"
# 4. (v37 æ–°å¢) èèµ„èåˆ¸å­—æ®µ
fields_margin = "ts_code,trade_date,rzye,rqye,rzmre,rzche,rqmcl,rqchl,rzrqye"

def init_model():
    """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
    global model, vol_multiplier
    
    if not HAS_MODEL or not HAS_FEATURE_ENGINE:
        return False
    
    if not os.path.exists(MODEL_PATH):
        print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}ï¼Œè·³è¿‡é¢„æµ‹åŠŸèƒ½", flush=True)
        return False

    try:
        model = cb.CatBoostClassifier()
        model.load_model(MODEL_PATH)
        
        if os.path.exists(PARAMS_PATH):
            with open(PARAMS_PATH, 'r') as f:
                params = json.load(f)
                vol_multiplier = params.get('vol_multiplier_best', 0.89)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ³¢åŠ¨ç‡ä¹˜æ•°: {vol_multiplier:.4f}", flush=True)
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", flush=True)
        return False

def predict_stock(ts_code, df):
    """å¯¹å•åªè‚¡ç¥¨è¿›è¡Œé¢„æµ‹ (å¤šç©ºåŒå‘)"""
    global report, count_debug, all_predictions
    
    if model is None or len(df) < 60:
        if len(df) < 60:
            print(f"  [{ts_code}] æ•°æ®ä¸è¶³ ({len(df)}è¡Œ)ï¼Œè·³è¿‡é¢„æµ‹", flush=True)
        return

    try:
        # ç‰¹å¾å·¥ç¨‹
        # 1. åŸºç¡€ç‰¹å¾ (shared)
        # (v32: shared/features.py ç°åœ¨åŒ…å«æ‰€æœ‰ç‰¹å¾é€»è¾‘ï¼ŒåŒ…æ‹¬ close_lag1, volume_change ç­‰)
        df_features = apply_technical_indicators(df)
        
        # 2. (v32: ç§»é™¤æ‰‹åŠ¨è¡¥å…¨ï¼Œå·²ç»Ÿä¸€åˆ° shared/features.py)
        # if 'close_lag1' not in df_features.columns: ...
        
        latest_row = df_features.iloc[[-1]].copy()
        current_date = latest_row['trade_date'].values[0]
        
        # é¢„æµ‹
        # v28 ä¿®å¤: CatBoost é¢„æµ‹æ—¶ç‰¹å¾é¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
        # è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº (å‚è€ƒ training/train.py get_feature_columns)
        # è¿™é‡Œæˆ‘ä»¬åŠ¨æ€è·å–æ¨¡å‹éœ€è¦çš„ç‰¹å¾å
        model_feature_names = model.feature_names_
        
        # æ£€æŸ¥ç¼ºå¤±ç‰¹å¾
        missing_features = [f for f in model_feature_names if f not in latest_row.columns]
        if missing_features:
            print(f"  [{ts_code}] ç¼ºå¤±ç‰¹å¾: {missing_features}ï¼Œè·³è¿‡", flush=True)
            return

        # æŒ‰æ¨¡å‹è¦æ±‚çš„é¡ºåºé‡æ’ç‰¹å¾
        X_predict = latest_row[model_feature_names]
        
        prob = model.predict_proba(X_predict)[0]
        prob_down, prob_flat, prob_up = prob[0], prob[1], prob[2]
        
        # è°ƒè¯•è¾“å‡º
        count_debug += 1
        # å¦‚æœæ€»æ•°å°‘äº 200 (è°ƒè¯•æ¨¡å¼)ï¼Œåˆ™æ‰“å°æ‰€æœ‰é¢„æµ‹ç»“æœ
        if len(report) < 200 or count_debug <= 5 or count_debug % 200 == 0 or prob_up > 0.25:
            print(f"  [{ts_code}] é¢„æµ‹: è·Œ{prob_down:.2f} å¹³{prob_flat:.2f} æ¶¨{prob_up:.2f}", flush=True)

        # ç­–ç•¥é€»è¾‘
        current_vol = latest_row['volatility_factor'].values[0]
        if pd.isna(current_vol): 
            current_vol = 0.02
        
        implied_return = current_vol * vol_multiplier
        signal = "âšª è§‚æœ›"
        position = 0.0
        reason = ""
        is_candidate = False
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ (Debugç”¨)
        all_predictions.append({
            'ä»£ç ': ts_code,
            'æ—¥æœŸ': pd.to_datetime(current_date).strftime('%Y-%m-%d'),
            'ä¿¡å·': "Debug",
            'ä¸Šæ¶¨æ¦‚ç‡': f"{prob_up:.1%}",
            'ä¸‹è·Œæ¦‚ç‡': f"{prob_down:.1%}",
            'æ³¢åŠ¨ç‡': f"{current_vol:.1%}",
            'é¢„æœŸæ”¶ç›Š': f"{implied_return:.1%}",
            'å»ºè®®ä»“ä½': "0.0%",
            'ç†ç”±': "Debugè®°å½•",
            'prob_up_raw': prob_up,
            'prob_down_raw': prob_down,
            'max_prob': max(prob_up, prob_down)
        })
        
        # --- é˜ˆå€¼è®¾ç½® (åŸºäºæœ€æ–°æ¨¡å‹åˆ†æ: Down å‡†, Up ä¿å®ˆ) ---
        THRESHOLD_WATCH_UP = 0.28    # é™ä½åšå¤šé—¨æ§›
        THRESHOLD_STRONG_UP = 0.38   # å¼ºåŠ›åšå¤šé—¨æ§›
        THRESHOLD_WATCH_DOWN = 0.35  # åšç©ºé—¨æ§›
        THRESHOLD_STRONG_DOWN = 0.45 # å¼ºåŠ›åšç©ºé—¨æ§›

        # --- 1. åšå¤šä¿¡å· (Long) ---
        if prob_up > prob_down and prob_up > THRESHOLD_WATCH_UP:
            signal = "ğŸ”µ å…³æ³¨å¤š"
            reason = f"çœ‹æ¶¨({prob_up:.1%})"
            is_candidate = True
            
            # å¼ºåŠ›ä¹°å…¥æ¡ä»¶
            if prob_up > THRESHOLD_STRONG_UP and prob_up > prob_flat:
                if implied_return > MIN_RETURN_THRESHOLD:
                    signal = "ğŸ”´ å¼ºåŠ›åšå¤š"
                    position = min(1.0, 0.02 / (current_vol + 1e-5))
                    reason = f"é«˜èƒœç‡({prob_up:.0%}) é«˜èµ”ç‡(>{implied_return:.1%})"
                else:
                    signal = "ğŸŸ  æ½œä¼åšå¤š" # èƒœç‡é«˜ä½†æ³¢åŠ¨ç‡ä½
                    reason = f"é«˜èƒœç‡({prob_up:.0%}) ä½æ³¢åŠ¨"

        # --- 2. åšç©ºä¿¡å· (Short) ---
        elif prob_down > prob_up and prob_down > THRESHOLD_WATCH_DOWN:
            signal = "ğŸŸ¡ å…³æ³¨ç©º"
            reason = f"çœ‹è·Œ({prob_down:.1%})"
            is_candidate = True
            
            if prob_down > THRESHOLD_STRONG_DOWN and prob_down > prob_flat:
                signal = "ğŸŸ¢ å¼ºåŠ›åšç©º"
                reason = f"é«˜ç¡®ä¿¡åº¦({prob_down:.1%})"
                position = min(1.0, 0.02 / (current_vol + 1e-5))

        if is_candidate:
            item = {
                'ä»£ç ': ts_code,
                'æ—¥æœŸ': pd.to_datetime(current_date).strftime('%Y-%m-%d'),
                'ä¿¡å·': signal,
                'ä¸Šæ¶¨æ¦‚ç‡': f"{prob_up:.1%}",
                'ä¸‹è·Œæ¦‚ç‡': f"{prob_down:.1%}",
                'æ³¢åŠ¨ç‡': f"{current_vol:.1%}",
                'é¢„æœŸæ”¶ç›Š': f"{implied_return:.1%}",
                'å»ºè®®ä»“ä½': f"{position:.1%}",
                'ç†ç”±': reason,
                'prob_up_raw': prob_up,
                'prob_down_raw': prob_down,
                'max_prob': max(prob_up, prob_down)
            }
            report.append(item)
            if "å¼ºåŠ›" in signal:
                print(f"  !!! å‘ç°æœºä¼š [{ts_code}]: {signal} - {reason}", flush=True)

    except Exception as e:
        print(f"âŒ [{ts_code}] é¢„æµ‹å‡ºé”™: {e}", flush=True)
        import traceback
        traceback.print_exc()

def generate_report(missing_features_info=None):
    """ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š (åˆ†å¤šç©ºå±•ç¤º)"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    report_path = "reports/strategy_report.md"
    
    if report:
        df_report = pd.DataFrame(report)
        
        # åˆ†ç¦»å¤šç©º
        df_long = df_report[df_report['ä¿¡å·'].str.contains('å¤š')].sort_values('prob_up_raw', ascending=False)
        df_short = df_report[df_report['ä¿¡å·'].str.contains('ç©º')].sort_values('prob_down_raw', ascending=False)
        
        # ç§»é™¤åŸå§‹æ’åºåˆ—ï¼Œä¿æŒè¡¨æ ¼æ•´æ´
        cols_to_drop = ['prob_up_raw', 'prob_down_raw', 'max_prob']
        df_long_display = df_long.drop(columns=cols_to_drop, errors='ignore')
        df_short_display = df_short.drop(columns=cols_to_drop, errors='ignore')
        
        print(f"\n=== æ¯æ—¥ç­–ç•¥æŠ¥å‘Š (æ€»è®¡: {len(df_report)}) ===", flush=True)
        print(f"å¤šå¤´æœºä¼š: {len(df_long)} | ç©ºå¤´æœºä¼š: {len(df_short)}", flush=True)
        
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        with open(report_path, "w") as f:
            f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str})\n\n")
            
            # --- (æ–°å¢) ç³»ç»ŸçŠ¶æ€/æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š ---
            if missing_features_info:
                f.write("## âš ï¸ ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š\n")
                f.write(f"**æ•°æ®å®Œæ•´æ€§**: {missing_features_info['status']}\n")
                if missing_features_info['missing']:
                    f.write(f"**ç¼ºå¤±æ•°æ®æº**: {', '.join(missing_features_info['missing'])}\n")
                    f.write("> æ³¨æ„: ç¼ºå¤±æ•°æ®å¯èƒ½å¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™ (å¦‚ç¼ºå¤±èµ„é‡‘æµæ•°æ®)ã€‚\n\n")
                else:
                    f.write("> âœ… æ‰€æœ‰å…³é”®æ•°æ®æºå‡å·²è¿æ¥ã€‚\n\n")
            # ------------------------------------

            f.write(f"**æ€»è®¡å…¥é€‰**: {len(df_report)} (å¤šå¤´: {len(df_long)}, ç©ºå¤´: {len(df_short)})\n\n")
            
            f.write("## ğŸ”´ å¤šå¤´æœºä¼š (Top 50)\n")
            if not df_long.empty:
                f.write(df_long_display.head(50).to_markdown(index=False))
            else:
                f.write("æ— ç¬¦åˆæ¡ä»¶çš„å¤šå¤´æœºä¼šã€‚\n")
            
            f.write("\n\n## ğŸŸ¢ ç©ºå¤´æœºä¼š (Top 50)\n")
            if not df_short.empty:
                f.write(df_short_display.head(50).to_markdown(index=False))
            else:
                f.write("æ— ç¬¦åˆæ¡ä»¶çš„ç©ºå¤´æœºä¼šã€‚\n")
        
        csv_path = report_path.replace(".md", ".csv")
        # CSV ä¿ç•™åŸå§‹æ¦‚ç‡åˆ—ï¼Œæ–¹ä¾¿ç”¨æˆ·è‡ªå·±æ’åº
        df_report.sort_values('max_prob', ascending=False).to_csv(csv_path, index=False)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path} å’Œ {csv_path}", flush=True)
    else:
        print("â„¹ï¸ ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“æœºä¼š", flush=True)
        
        # --- å¼ºåˆ¶è¾“å‡º Top 10 (Debug) ---
        if all_predictions:
            print("âš ï¸ å¼ºåˆ¶è¾“å‡º Top 10 é¢„æµ‹ç»“æœ (å³ä½¿æœªè¾¾é˜ˆå€¼)", flush=True)
            df_all = pd.DataFrame(all_predictions)
            # æŒ‰æœ€å¤§æ¦‚ç‡æ’åº
            df_top = df_all.sort_values('max_prob', ascending=False).head(10)
            
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            with open(report_path, "w") as f:
                f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str}) - DEBUG MODE\n\n")
                
                # --- (æ–°å¢) ç³»ç»ŸçŠ¶æ€/æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š ---
                if missing_features_info:
                    f.write("## âš ï¸ ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š\n")
                    f.write(f"**æ•°æ®å®Œæ•´æ€§**: {missing_features_info['status']}\n")
                    if missing_features_info['missing']:
                        f.write(f"**ç¼ºå¤±æ•°æ®æº**: {', '.join(missing_features_info['missing'])}\n")
                # ------------------------------------

                f.write("âš ï¸ **æ³¨æ„**: ä»Šæ—¥æ— ç¬¦åˆé˜ˆå€¼çš„æœºä¼šã€‚ä»¥ä¸‹ä¸ºæ¦‚ç‡æœ€é«˜çš„ Top 10 è‚¡ç¥¨ (ä»…ä¾›è°ƒè¯•å‚è€ƒ)ã€‚\n\n")
                f.write(df_top.drop(columns=['prob_up_raw', 'prob_down_raw', 'max_prob'], errors='ignore').to_markdown(index=False))
            
            print(f"âœ… å¼ºåˆ¶æŠ¥å‘Šå·²ä¿å­˜: {report_path}", flush=True)
        else:
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            with open(report_path, "w") as f:
                f.write(f"# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š ({today_str})\n\n")
                f.write("ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“æœºä¼šï¼Œä¸”æ— ä»»ä½•æœ‰æ•ˆé¢„æµ‹æ•°æ®ã€‚")

def get_hist(ts_code: str):
    """
    è·å–å…¨é‡æ•°æ®ï¼šè¡Œæƒ… + æ¯æ—¥æŒ‡æ ‡ + èµ„é‡‘æµå‘
    å¹¶åˆå¹¶ä¸ºä¸€ä¸ª DataFrame
    """
    try:
        # 1. è·å–æ—¥çº¿è¡Œæƒ…
        df_daily = pro.daily(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_daily)
        if df_daily.empty:
            return None
            
        # 2. è·å–æ¯æ—¥æŒ‡æ ‡ (å¸‚å€¼, æ¢æ‰‹, PE/PB)
        # æ³¨æ„: æ¥å£å¯èƒ½è¿”å›ç©ºï¼Œéœ€å¤„ç†
        # try:
        #     df_basic = pro.daily_basic(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_daily_basic)
        # except Exception:
        df_basic = pd.DataFrame()
            
        # 3. è·å–èµ„é‡‘æµå‘
        try:
            # moneyflow æœ‰æ—¶åœ¨ç½‘ç»œæˆ–æœåŠ¡ç«¯è¾ƒæ…¢ï¼Œè®¾ç½®çŸ­è¶…æ—¶ä¿æŠ¤
            # xcsc_tushare çš„ pro.moneyflow æœ¬èº«æ—  timeout å‚æ•°ï¼Œå› æ­¤ä½¿ç”¨çº¿ç¨‹åŒ…è£…ä»¥é¿å…é˜»å¡
            import concurrent.futures

            def call_moneyflow():
                return pro.moneyflow(ts_code=ts_code, start_date=START_DATE, end_date="", fields=fields_moneyflow)

            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(call_moneyflow)
                try:
                    df_flow = future.result(timeout=10)  # 10ç§’è¶…æ—¶
                except Exception:
                    future.cancel()
                    df_flow = pd.DataFrame()
        except Exception:
            df_flow = pd.DataFrame()

        # --- æ•°æ®åˆå¹¶ ---
        # ä»¥ daily ä¸ºä¸»è¡¨ï¼Œå·¦è¿æ¥å…¶ä»–è¡¨
        df_merge = df_daily
        
        if not df_basic.empty:
            df_merge = pd.merge(df_merge, df_basic, on=['ts_code', 'trade_date'], how='left')
            
        if not df_flow.empty:
            df_merge = pd.merge(df_merge, df_flow, on=['ts_code', 'trade_date'], how='left')

        # ç»Ÿä¸€æŒ‰æ—¥æœŸæ’åº (æ—§åˆ°æ–°)
        df_merge = df_merge.sort_values('trade_date').reset_index(drop=True)
        # ç¡®ä¿æ—¥æœŸæ ¼å¼
        df_merge["trade_date"] = pd.to_datetime(df_merge["trade_date"], format="%Y%m%d")

        # åœ¨è¿‡æ»¤åœç‰Œä¹‹å‰ï¼Œå°è¯•æ‹‰å–å¹¶åˆå¹¶å­£æŠ¥è´¢åŠ¡æ•°æ®ï¼ˆå®‰å…¨å¯¹é½ï¼‰
        try:
            from shared.financials import fetch_financials, align_financials_to_daily
            df_fin = fetch_financials(pro, ts_code, start_date=START_DATE)
            if df_fin is not None and not df_fin.empty:
                aligned_fin = align_financials_to_daily(df_merge, df_fin)
                # åˆå¹¶åˆ°ä¸»è¡¨ï¼ˆæŒ‰è¡Œå¯¹é½ï¼‰ï¼Œä¿ç•™è´¢åŠ¡å­—æ®µ
                try:
                    df_merge = pd.concat([df_merge.reset_index(drop=True), aligned_fin.reset_index(drop=True)], axis=1)
                except Exception:
                    pass
        except Exception:
            # è´¢åŠ¡æŠ“å–éå…³é”®ï¼Œå¤±è´¥æ—¶ç»§ç»­
            pass

        # è¿‡æ»¤åœç‰Œæ—¥å¹¶å¯¹åˆšå¤ç‰Œçš„è‚¡ç¥¨åšçŸ­æœŸä¿æŠ¤ï¼ˆé˜²æ­¢å¤ç‰Œå¼‚å¸¸æ³¢åŠ¨æ±¡æŸ“ç‰¹å¾ï¼‰
        RESUME_SAFE_DAYS = 5
        if 'trade_status' in df_merge.columns:
            # æ ‡è®°æ˜¯å¦äº¤æ˜“ï¼šXCSC ä½¿ç”¨ä¸­æ–‡å­—æ®µå€¼ï¼ˆä¾‹å¦‚ 'äº¤æ˜“'ã€'åœç‰Œ'ã€'XD' ç­‰ï¼‰
            # å…¼å®¹å¸¸è§å€¼ï¼š'T','äº¤æ˜“','TRADE','äº¤æ˜“ä¸­','1'
            valid_trade_vals = set(['T', 'äº¤æ˜“', 'TRADE', 'äº¤æ˜“ä¸­', '1'])
            # æœ‰äº›å€¼å¯èƒ½åŒ…å«ç©ºæ ¼æˆ–å¤§å°å†™å·®å¼‚ï¼Œç»Ÿä¸€å¤„ç†
            def is_trade_val(x):
                try:
                    s = str(x).strip()
                except Exception:
                    return False
                return s in valid_trade_vals or s == 'äº¤æ˜“'
            df_merge['is_trade'] = df_merge['trade_status'].apply(is_trade_val)
            # å°†è¿ç»­æ®µåˆ†ç»„
            grp = (df_merge['is_trade'] != df_merge['is_trade'].shift(fill_value=df_merge['is_trade'].iloc[0])).cumsum()
            df_merge['grp'] = grp
            df_merge['days_since_resume'] = 0
            grp_vals = df_merge.groupby('grp')['is_trade'].first().to_dict()
            groups = sorted(grp_vals.items(), key=lambda x: x[0])
            # iterate groups to find resume groups (group with is_trade==True and previous group is_trade==False)
            for idx in range(1, len(groups)):
                gid, val = groups[idx]
                prev_gid, prev_val = groups[idx-1]
                if val and not prev_val:
                    # this group is a resume after suspension
                    mask = df_merge['grp'] == gid
                    n = mask.sum()
                    # set 1..n
                    df_merge.loc[mask, 'days_since_resume'] = list(range(1, n+1))
            # åˆ é™¤éäº¤æ˜“æ—¥
            df_merge = df_merge[df_merge['is_trade']]
            # åˆ é™¤å¤ç‰ŒåçŸ­æœŸæ•°æ®
            df_merge = df_merge[~((df_merge['days_since_resume'] > 0) & (df_merge['days_since_resume'] <= RESUME_SAFE_DAYS))]
            # æ¸…ç†è¾…åŠ©åˆ—
            df_merge.drop(columns=['is_trade', 'grp', 'days_since_resume'], inplace=True, errors='ignore')

        # --- å•ä½æ ¡æ­£: æ¨æ–­å¹¶ç»Ÿä¸€é‡/é¢å•ä½åˆ°â€œè‚¡â€å’Œäººæ°‘å¸é‡‘é¢ ---
        try:
            if 'volume' in df_merge.columns and 'amount' in df_merge.columns and 'close' in df_merge.columns:
                mask = df_merge['volume'].notna() & df_merge['amount'].notna() & df_merge['close'].notna() & (df_merge['close']>0) & (df_merge['volume']>0)
                if mask.sum() >= 5:
                    ratios = (df_merge.loc[mask, 'amount'] / (df_merge.loc[mask, 'volume'] * df_merge.loc[mask, 'close'] + 1e-12)).replace([float('inf'), -float('inf')], pd.NA).dropna()
                    if len(ratios) >= 3:
                        scale = float(ratios.median())
                        if 1e-6 < scale < 1e6:
                            # æ ‡å‡†åŒ–åˆ—
                            df_merge['volume_shares'] = df_merge['volume'] * scale
                            df_merge['amount_cny'] = df_merge['volume_shares'] * df_merge['close']
                            df_merge['volume_scale_inferred'] = scale
                            # moneyflow é‡‘é¢åˆ—è°ƒæ•´
                            if 'net_mf_amount' in df_merge.columns and df_merge['net_mf_amount'].notna().sum() > 0:
                                orig_med = df_merge.loc[mask, 'amount'].median()
                                new_med = df_merge.loc[mask, 'amount_cny'].median()
                                if orig_med and abs(orig_med) > 0:
                                    amt_factor = new_med / orig_med
                                    df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount'] * amt_factor
                                else:
                                    df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount']
        except Exception:
            # å•ä¸ªç¥¨çš„å½’ä¸€åŒ–å¤±è´¥ä¸åº”ä¸­æ–­æ•´ä¸ªæµç¨‹
            pass

        # ä»…è¿›è¡Œé™ç²¾åº¦å¤„ç†ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–ç‰¹å¾ï¼Œä¿æŒæ•°æ®çº¯æ´
        df_merge = downcast(df_merge)

        if len(df_merge) > 21:
            return ts_code, df_merge
        else:
            return None

    except Exception as e:
        raise e

def list_main_board_cs():
    """è·å–ä¸»æ¿å·²ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨"""
    today = datetime.today().strftime("%Y%m%d")
    temp0 = pro.stock_basic(market="CS", fields="ts_code,name,list_date,delist_date,list_board_name")
    temp0 = temp0[temp0["delist_date"].isna()]  # æœªé€€å¸‚
    temp0 = temp0[temp0["list_board_name"] == "ä¸»æ¿"]  # ä¸»æ¿
    temp0 = temp0[temp0["list_date"] <= today]  # å·²ç»ä¸Šå¸‚
    return temp0[["ts_code", "name"]].reset_index(drop=True)

# (v32: å·²ç§»é™¤ add_features å‡½æ•°ï¼Œç¡®ä¿ä¿å­˜çš„æ•°æ®åªåŒ…å«åŸå§‹è¡Œæƒ…)

def downcast(df: pd.DataFrame) -> pd.DataFrame:
    """é™ä½æµ®ç‚¹ç²¾åº¦ä»¥èŠ‚çœç©ºé—´"""
    for col in df.select_dtypes(include=["float64"]).columns:
        df[col] = df[col].astype("float32")
    return df


def merge_and_postprocess(ts_code: str, df_daily, df_basic, df_flow, df_margin=None, df_top_list=None, df_block_trade=None):
    """
    ç»Ÿä¸€çš„æ•°æ®åˆå¹¶ä¸åå¤„ç†é€»è¾‘ï¼š
    1. åˆå¹¶ daily, daily_basic, moneyflow, margin_detail
    2. (v37) åˆå¹¶é¾™è™æ¦œã€å¤§å®—äº¤æ˜“æ•°æ®
    3. å¯¹é½è´¢åŠ¡æ•°æ®
    4. è¿‡æ»¤åœç‰Œæ—¥ä¸å¤ç‰Œä¿æŠ¤æœŸ
    5. å•ä½å½’ä¸€åŒ–
    6. é™ç²¾åº¦
    è¿”å›å¤„ç†åçš„ DataFrameï¼Œè‹¥æ•°æ®ä¸è¶³åˆ™è¿”å› None
    
    v37 æ›´æ–°: 
    - æ–°å¢ df_margin å‚æ•° (èèµ„èåˆ¸æ•°æ®)
    - æ–°å¢ df_top_list å‚æ•° (é¾™è™æ¦œæ•°æ®ï¼Œå·²æŒ‰ ts_code è¿‡æ»¤)
    - æ–°å¢ df_block_trade å‚æ•° (å¤§å®—äº¤æ˜“æ•°æ®ï¼Œå·²æŒ‰ ts_code è¿‡æ»¤)
    """
    if df_daily is None or df_daily.empty:
        return None

    df_merge = df_daily
    merge_keys = ['ts_code', 'trade_date']
    base_cols = set(df_merge.columns)
    
    if df_basic is not None and not df_basic.empty:
        # å»é™¤ä¸ df_daily é‡å¤çš„åˆ— (é™¤äº† merge keys)
        dup_cols = [c for c in df_basic.columns if c in base_cols and c not in merge_keys]
        if dup_cols:
            df_basic = df_basic.drop(columns=dup_cols)
        df_merge = pd.merge(df_merge, df_basic, on=merge_keys, how='left')
        base_cols = set(df_merge.columns)
    
    if df_flow is not None and not df_flow.empty:
        # å»é™¤ä¸å·²åˆå¹¶æ•°æ®é‡å¤çš„åˆ— (é™¤äº† merge keys)
        dup_cols = [c for c in df_flow.columns if c in base_cols and c not in merge_keys]
        if dup_cols:
            df_flow = df_flow.drop(columns=dup_cols)
        df_merge = pd.merge(df_merge, df_flow, on=merge_keys, how='left')
        base_cols = set(df_merge.columns)
    
    # (v37 æ–°å¢) åˆå¹¶èèµ„èåˆ¸æ•°æ®
    if df_margin is not None and not df_margin.empty:
        dup_cols = [c for c in df_margin.columns if c in base_cols and c not in merge_keys]
        if dup_cols:
            df_margin = df_margin.drop(columns=dup_cols)
        df_merge = pd.merge(df_merge, df_margin, on=merge_keys, how='left')
        base_cols = set(df_merge.columns)
    
    # (v37 æ–°å¢) åˆå¹¶é¾™è™æ¦œæ•°æ®
    if df_top_list is not None and not df_top_list.empty:
        # é¾™è™æ¦œå…³é”®å­—æ®µ (æ¥è‡ª top_list è¡¨):
        # - l_buy: é¾™è™æ¦œä¹°å…¥é¢
        # - l_sell: é¾™è™æ¦œå–å‡ºé¢
        # - net_amount: å‡€ä¹°å…¥é¢
        # å…ˆèšåˆåŒä¸€å¤©çš„å¤šæ¡è®°å½• (åŒä¸€åªè‚¡å¯èƒ½å¤šæ¬¡ä¸Šæ¦œ)
        agg_dict = {'net_amount': 'sum'}
        if 'l_buy' in df_top_list.columns:
            agg_dict['l_buy'] = 'sum'
        if 'l_sell' in df_top_list.columns:
            agg_dict['l_sell'] = 'sum'
        
        top_agg = df_top_list.groupby(['ts_code', 'trade_date']).agg(agg_dict).reset_index()
        
        # é‡å‘½ååˆ—ä»¥é¿å…ä¸å…¶ä»–æ•°æ®æºå†²çª
        rename_map = {'net_amount': 'top_net_amount'}
        if 'l_buy' in top_agg.columns:
            rename_map['l_buy'] = 'top_buy_amount'
        if 'l_sell' in top_agg.columns:
            rename_map['l_sell'] = 'top_sell_amount'
        top_agg.rename(columns=rename_map, inplace=True)
        
        # æ·»åŠ ä¸Šæ¦œæ¬¡æ•°
        top_count = df_top_list.groupby(['ts_code', 'trade_date']).size().reset_index(name='top_count')
        top_agg = pd.merge(top_agg, top_count, on=['ts_code', 'trade_date'], how='left')
        
        dup_cols = [c for c in top_agg.columns if c in base_cols and c not in merge_keys]
        if dup_cols:
            top_agg = top_agg.drop(columns=dup_cols)
        df_merge = pd.merge(df_merge, top_agg, on=merge_keys, how='left')
        base_cols = set(df_merge.columns)
    
    # (v37 æ–°å¢) åˆå¹¶å¤§å®—äº¤æ˜“æ•°æ®
    if df_block_trade is not None and not df_block_trade.empty:
        # å¤§å®—äº¤æ˜“å…³é”®å­—æ®µ: vol, amount, price
        # å…ˆèšåˆåŒä¸€å¤©çš„å¤šç¬”å¤§å®—äº¤æ˜“
        block_agg = df_block_trade.groupby(['ts_code', 'trade_date']).agg({
            'vol': 'sum',     # æˆäº¤é‡
            'amount': 'sum',  # æˆäº¤é¢
            'price': 'mean'   # æˆäº¤å‡ä»·
        }).reset_index()
        block_agg.rename(columns={
            'vol': 'block_vol',
            'amount': 'block_amount',
            'price': 'block_avg_price'
        }, inplace=True)
        
        # æ·»åŠ å¤§å®—äº¤æ˜“ç¬”æ•°
        block_count = df_block_trade.groupby(['ts_code', 'trade_date']).size().reset_index(name='block_count')
        block_agg = pd.merge(block_agg, block_count, on=['ts_code', 'trade_date'], how='left')
        
        dup_cols = [c for c in block_agg.columns if c in base_cols and c not in merge_keys]
        if dup_cols:
            block_agg = block_agg.drop(columns=dup_cols)
        df_merge = pd.merge(df_merge, block_agg, on=merge_keys, how='left')

    df_merge = df_merge.sort_values('trade_date').reset_index(drop=True)
    try:
        df_merge['trade_date'] = pd.to_datetime(df_merge['trade_date'], format="%Y%m%d")
    except Exception:
        pass

    # è´¢åŠ¡æ•°æ®å¯¹é½ï¼ˆå¯é€šè¿‡ SKIP_FINANCIALS=1 è·³è¿‡ä»¥åŠ é€Ÿï¼‰
    if os.environ.get('SKIP_FINANCIALS', '0') != '1':
        try:
            from shared.financials import fetch_financials, align_financials_to_daily
            df_fin = fetch_financials(pro, ts_code, start_date=START_DATE)
            if df_fin is not None and not df_fin.empty:
                aligned_fin = align_financials_to_daily(df_merge, df_fin)
                try:
                    df_merge = pd.concat([df_merge.reset_index(drop=True), aligned_fin.reset_index(drop=True)], axis=1)
                except Exception:
                    pass
        except Exception:
            pass

    # åœç‰Œè¿‡æ»¤ä¸å¤ç‰Œä¿æŠ¤
    RESUME_SAFE_DAYS = 5
    if 'trade_status' in df_merge.columns:
        valid_trade_vals = {'T', 'äº¤æ˜“', 'TRADE', 'äº¤æ˜“ä¸­', '1'}
        def is_trade_val(x):
            try:
                return str(x).strip() in valid_trade_vals
            except Exception:
                return False
        df_merge['is_trade'] = df_merge['trade_status'].apply(is_trade_val)
        grp = (df_merge['is_trade'] != df_merge['is_trade'].shift(fill_value=df_merge['is_trade'].iloc[0])).cumsum()
        df_merge['grp'] = grp
        df_merge['days_since_resume'] = 0
        grp_vals = df_merge.groupby('grp')['is_trade'].first().to_dict()
        groups = sorted(grp_vals.items(), key=lambda x: x[0])
        for idx in range(1, len(groups)):
            gid, val = groups[idx]
            prev_gid, prev_val = groups[idx-1]
            if val and not prev_val:
                mask = df_merge['grp'] == gid
                df_merge.loc[mask, 'days_since_resume'] = list(range(1, mask.sum()+1))
        df_merge = df_merge[df_merge['is_trade']]
        df_merge = df_merge[~((df_merge['days_since_resume'] > 0) & (df_merge['days_since_resume'] <= RESUME_SAFE_DAYS))]
        df_merge.drop(columns=['is_trade', 'grp', 'days_since_resume'], inplace=True, errors='ignore')

    # å•ä½å½’ä¸€åŒ–
    try:
        if 'volume' in df_merge.columns and 'amount' in df_merge.columns and 'close' in df_merge.columns:
            mask = df_merge['volume'].notna() & df_merge['amount'].notna() & df_merge['close'].notna() & (df_merge['close']>0) & (df_merge['volume']>0)
            if mask.sum() >= 5:
                ratios = (df_merge.loc[mask, 'amount'] / (df_merge.loc[mask, 'volume'] * df_merge.loc[mask, 'close'] + 1e-12)).replace([float('inf'), -float('inf')], pd.NA).dropna()
                if len(ratios) >= 3:
                    scale = float(ratios.median())
                    if 1e-6 < scale < 1e6:
                        df_merge['volume_shares'] = df_merge['volume'] * scale
                        df_merge['amount_cny'] = df_merge['volume_shares'] * df_merge['close']
                        df_merge['volume_scale_inferred'] = scale
                        if 'net_mf_amount' in df_merge.columns and df_merge['net_mf_amount'].notna().sum() > 0:
                            orig_med = df_merge.loc[mask, 'amount'].median()
                            new_med = df_merge.loc[mask, 'amount_cny'].median()
                            if orig_med and abs(orig_med) > 0:
                                df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount'] * (new_med / orig_med)
                            else:
                                df_merge['net_mf_amount_cny'] = df_merge['net_mf_amount']
    except Exception:
        pass

    df_merge = downcast(df_merge)
    return df_merge if len(df_merge) > 21 else None


def main():
    # å¹¶è¡Œå¤„ç†é…ç½®
    parallel_workers = int(os.environ.get('PARALLEL_WORKERS', '2'))
    skip_fin = os.environ.get('SKIP_FINANCIALS', '0') == '1'
    print(f"ğŸš€ å¯åŠ¨æ•°æ®ä¸‹è½½ä¸é¢„æµ‹è„šæœ¬ (å¹¶è¡Œ={parallel_workers}, è·³è¿‡è´¢åŠ¡={skip_fin})...", flush=True)
    os.makedirs(OUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    model_enabled = init_model()
    if model_enabled:
        print("ğŸ“Š é¢„æµ‹åŠŸèƒ½å·²å¯ç”¨", flush=True)
    else:
        print("ğŸ“Š é¢„æµ‹åŠŸèƒ½æœªå¯ç”¨ï¼Œä»…ä¸‹è½½æ•°æ®", flush=True)
    
    # --- (v37 æ–°å¢) è·å–å…¨å¸‚åœºé¾™è™æ¦œå’Œå¤§å®—äº¤æ˜“æ•°æ® ---
    # è¿™äº›æ•°æ®æŒ‰æ—¥æœŸè·å–ï¼Œè€ŒéæŒ‰ä¸ªè‚¡ï¼Œæ‰€ä»¥åœ¨æ‰¹é‡å¤„ç†å‰ä¸€æ¬¡æ€§è·å–
    from shared.downloader import fetch_market_data_by_date
    
    # è·å–æœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®ï¼ˆç”¨äºå†å²å›å¡«ï¼‰
    # å®é™…ç”Ÿäº§ä¸­å¯ä»¥åªè·å–å½“å¤©æ•°æ®
    market_data_cache = {}  # {trade_date: {'top_list': df, 'block_trade': df}}
    
    try:
        # è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥
        today = datetime.today().strftime("%Y%m%d")
        trade_cal = pro.trade_cal(exchange='SSE', start_date=(datetime.today() - timedelta(days=30)).strftime("%Y%m%d"), end_date=today)
        if trade_cal is not None and not trade_cal.empty:
            recent_trade_dates = trade_cal[trade_cal['is_open'] == 1]['cal_date'].sort_values(ascending=False).head(5).tolist()
            
            print(f"ğŸ“¡ è·å–æœ€è¿‘ {len(recent_trade_dates)} ä¸ªäº¤æ˜“æ—¥çš„é¾™è™æ¦œ/å¤§å®—æ•°æ®...", flush=True)
            for td in recent_trade_dates:
                try:
                    mkt_data = fetch_market_data_by_date(pro, td)
                    if mkt_data:
                        market_data_cache[td] = mkt_data
                        top_cnt = len(mkt_data.get('top_list', pd.DataFrame()))
                        block_cnt = len(mkt_data.get('block_trade', pd.DataFrame()))
                        if top_cnt > 0 or block_cnt > 0:
                            print(f"    {td}: é¾™è™æ¦œ {top_cnt} æ¡, å¤§å®— {block_cnt} æ¡", flush=True)
                except Exception as e:
                    print(f"    {td}: è·å–å¤±è´¥ ({e})", flush=True)
    except Exception as e:
        print(f"âš ï¸ è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}ï¼Œå°†è·³è¿‡é¾™è™æ¦œ/å¤§å®—ç‰¹å¾", flush=True)
    # --- å…¨å¸‚åœºæ•°æ®è·å–ç»“æŸ ---
    
    print("ğŸ“‹ æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...", flush=True)
    try:
        ts_codes = list_main_board_cs()
    except Exception as e:
        print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}", flush=True)
        return

    # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼Œä¾¿äºæœ¬åœ°å¿«é€Ÿ smoke test
    # é»˜è®¤ 0 è¡¨ç¤ºå…¨é‡ä¸‹è½½ï¼›æœ¬åœ°æµ‹è¯•å¯è®¾ç½® MAX_TICKERS=20
    max_tickers = int(os.environ.get('MAX_TICKERS', '0'))
    if max_tickers and max_tickers > 0:
        ts_codes = ts_codes.head(max_tickers)
        print(f"âœ… è·å–åˆ° {len(ts_codes)} åªè‚¡ç¥¨ï¼Œå¼€å§‹å¤„ç†... (MAX_TICKERS={max_tickers})", flush=True)
    else:
        print(f"âœ… è·å–åˆ° {len(ts_codes)} åªè‚¡ç¥¨ï¼Œå¼€å§‹å…¨é‡å¤„ç†...", flush=True)
    
    total = len(ts_codes)
    skipped = []
    batch_size = 10  # æ¯æ‰¹ä¸‹è½½ 10 æ”¯è‚¡ç¥¨

    # å¯¼å…¥æ‰¹é‡ä¸‹è½½å‡½æ•°
    from shared.downloader import fetch_batch
    from concurrent.futures import ThreadPoolExecutor, as_completed

    tickers = list(ts_codes['ts_code'].values)
    
    total_download_time = 0.0
    total_process_time = 0.0
    
    # (v37 æ–°å¢) å°†å¸‚åœºæ•°æ®ç¼“å­˜è½¬æ¢ä¸ºæŒ‰è‚¡ç¥¨ä»£ç ç´¢å¼•
    # market_data_cache: {trade_date: {'top_list': df, 'block_trade': df}}
    # è½¬æ¢ä¸º: top_list_by_code = {ts_code: df}, block_trade_by_code = {ts_code: df}
    top_list_by_code = {}
    block_trade_by_code = {}
    
    for trade_date, mkt_data in market_data_cache.items():
        # å¤„ç†é¾™è™æ¦œæ•°æ®
        df_top = mkt_data.get('top_list')
        if df_top is not None and not df_top.empty and 'ts_code' in df_top.columns:
            for code in df_top['ts_code'].unique():
                code_data = df_top[df_top['ts_code'] == code].copy()
                if code not in top_list_by_code:
                    top_list_by_code[code] = code_data
                else:
                    top_list_by_code[code] = pd.concat([top_list_by_code[code], code_data], ignore_index=True)
        
        # å¤„ç†å¤§å®—äº¤æ˜“æ•°æ®
        df_block = mkt_data.get('block_trade')
        if df_block is not None and not df_block.empty and 'ts_code' in df_block.columns:
            for code in df_block['ts_code'].unique():
                code_data = df_block[df_block['ts_code'] == code].copy()
                if code not in block_trade_by_code:
                    block_trade_by_code[code] = code_data
                else:
                    block_trade_by_code[code] = pd.concat([block_trade_by_code[code], code_data], ignore_index=True)
    
    print(f"ğŸ“Š å¸‚åœºæ•°æ®ç´¢å¼•å®Œæˆ: é¾™è™æ¦œæ¶‰åŠ {len(top_list_by_code)} åªè‚¡ç¥¨, å¤§å®—äº¤æ˜“æ¶‰åŠ {len(block_trade_by_code)} åªè‚¡ç¥¨", flush=True)
    
    # è¯»å–æ˜¯å¦è·³è¿‡é¢„æµ‹ï¼ˆç”¨äºåŠ é€Ÿçº¯ä¸‹è½½ä»»åŠ¡ï¼‰
    SKIP_PREDICTIONS = os.environ.get('SKIP_PREDICTIONS', '0') in ('1', 'true', 'True')

    # å®šä¹‰å•åªè‚¡ç¥¨çš„å¤„ç†å‡½æ•° (v37 æ›´æ–°: æ·»åŠ  top_list_by_code, block_trade_by_code)
    def process_one(code, daily_map, basic_map, flow_map, margin_map, top_list_by_code, block_trade_by_code):
        df_daily = daily_map.get(code)
        if df_daily is None or (hasattr(df_daily, 'empty') and df_daily.empty):
            return (code, False, 'no_data')
        
        df_basic = basic_map.get(code, pd.DataFrame())
        df_flow = flow_map.get(code, pd.DataFrame())
        df_margin = margin_map.get(code, pd.DataFrame())  # v37 æ–°å¢
        df_top_list = top_list_by_code.get(code, pd.DataFrame())  # v37 æ–°å¢
        df_block_trade = block_trade_by_code.get(code, pd.DataFrame())  # v37 æ–°å¢
        
        df_merge = merge_and_postprocess(code, df_daily, df_basic, df_flow, df_margin, df_top_list, df_block_trade)
        if df_merge is None:
            return (code, False, 'postprocess_fail')
        
        try:
            if not SKIP_PREDICTIONS and model_enabled and model is not None:
                predict_stock(code, df_merge.copy())
            out_file = os.path.join(OUT_DIR, f"{code}.parquet")
            df_merge.to_parquet(out_file, engine="pyarrow", compression="zstd", compression_level=3, index=False)
            return (code, True, None)
        except Exception as e:
            return (code, False, str(e))
    
    # å¼‚æ­¥ä¸‹è½½å‡½æ•° (v37 æ›´æ–°: æ·»åŠ  fields_margin)
    def download_batch(chunk):
        return fetch_batch(pro, chunk, START_DATE, fields_daily, fields_daily_basic, fields_moneyflow, fields_margin)
    
    # ä½¿ç”¨æµæ°´çº¿ï¼šä¸‹è½½å’Œå¤„ç†å¼‚æ­¥å¹¶è¡Œ
    # 1ä¸ªçº¿ç¨‹ç”¨äºé¢„å–ä¸‹ä¸€æ‰¹ï¼Œå…¶ä½™çº¿ç¨‹ç”¨äºå¤„ç†å½“å‰æ‰¹
    batches = [tickers[i:i+batch_size] for i in range(0, len(tickers), batch_size)]
    
    with ThreadPoolExecutor(max_workers=parallel_workers + 1) as executor:
        # é¢„å–ç¬¬ä¸€æ‰¹
        prefetch_future = executor.submit(download_batch, batches[0]) if batches else None
        
        for batch_idx, chunk in enumerate(batches):
            print(f"[{batch_idx * batch_size}/{total}] å¤„ç† {len(chunk)} æ”¯è‚¡ç¥¨...", flush=True)
            
            try:
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡çš„ä¸‹è½½å®Œæˆ
                t0 = time.time()
                if prefetch_future:
                    fetched = prefetch_future.result()
                else:
                    fetched = download_batch(chunk)
                download_time = time.time() - t0
                total_download_time += download_time
                
                # ç«‹å³å¯åŠ¨ä¸‹ä¸€æ‰¹çš„é¢„å–ï¼ˆå¦‚æœæœ‰ï¼‰
                next_batch_idx = batch_idx + 1
                if next_batch_idx < len(batches):
                    prefetch_future = executor.submit(download_batch, batches[next_batch_idx])
                else:
                    prefetch_future = None
                
                print(f"    â±ï¸ ä¸‹è½½è€—æ—¶: {download_time:.2f}s", flush=True)
                
                daily_map = fetched.get('daily', {})
                basic_map = fetched.get('daily_basic', {})
                flow_map = fetched.get('moneyflow', {})
                margin_map = fetched.get('margin', {})  # v37 æ–°å¢
                
                # å¹¶è¡Œå¤„ç†æœ¬æ‰¹è‚¡ç¥¨ (v37 æ›´æ–°: ä¼ é€’é¾™è™æ¦œå’Œå¤§å®—äº¤æ˜“æ•°æ®)
                t1 = time.time()
                process_futures = [executor.submit(process_one, code, daily_map, basic_map, flow_map, margin_map, top_list_by_code, block_trade_by_code) for code in chunk]
                for fut in as_completed(process_futures):
                    code, success, err = fut.result()
                    if not success:
                        if err and err != 'no_data' and err != 'postprocess_fail':
                            print(f"âŒ {code} å¤„ç†å‡ºé”™: {err}", flush=True)
                        skipped.append(code)
                
                process_time = time.time() - t1
                total_process_time += process_time
                print(f"    â±ï¸ å¤„ç†è€—æ—¶: {process_time:.2f}s (æœ¬æ‰¹å…± {download_time + process_time:.2f}s)", flush=True)
            except Exception as e:
                print(f"âŒ æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ°é€åªä¸‹è½½", flush=True)
                # å›é€€ï¼šé€åªä¸‹è½½
                for code in chunk:
                    try:
                        result = get_hist(code)
                        if result is None:
                            skipped.append(code)
                            continue
                        _, df = result
                        if not SKIP_PREDICTIONS and model_enabled and model is not None:
                            predict_stock(code, df.copy())
                        out_file = os.path.join(OUT_DIR, f"{code}.parquet")
                        df.to_parquet(out_file, engine="pyarrow", compression="zstd", compression_level=3, index=False)
                    except Exception as ee:
                        print(f"âŒ {code} å›é€€ä¸‹è½½å¤±è´¥: {ee}", flush=True)
                        skipped.append(code)

    # è¾“å‡ºæ€»è€—æ—¶ç»Ÿè®¡
    print(f"\nğŸ“Š è€—æ—¶ç»Ÿè®¡:", flush=True)
    print(f"    ä¸‹è½½æ€»è€—æ—¶: {total_download_time:.2f}s", flush=True)
    print(f"    å¤„ç†æ€»è€—æ—¶: {total_process_time:.2f}s", flush=True)
    print(f"    åˆè®¡: {total_download_time + total_process_time:.2f}s", flush=True)

    if skipped:
        pd.DataFrame(skipped, columns=["ts_code"]).to_csv("skipped.csv", index=False)
        print(f"âš ï¸ è·³è¿‡ {len(skipped)} ä¸ªè‚¡ç¥¨ï¼Œå·²å†™å…¥ skipped.csv", flush=True)

    # ç”Ÿæˆé¢„æµ‹æŠ¥å‘Šï¼ˆå¯é€‰ï¼ŒSKIP_PREDICTIONS=1 æ—¶è·³è¿‡ï¼‰
    if SKIP_PREDICTIONS:
        print("â„¹ï¸ SKIP_PREDICTIONS=1ï¼Œå·²è·³è¿‡é¢„æµ‹ä¸æŠ¥å‘Šç”Ÿæˆ", flush=True)
        # ç”Ÿæˆå ä½æŠ¥å‘Šï¼Œé˜²æ­¢ GitHub Actions æŠ¥é”™
        report_path = "reports/strategy_report.md"
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        with open(report_path, "w") as f:
            f.write("# æ¯æ—¥é‡åŒ–ç­–ç•¥æŠ¥å‘Š (å·²è·³è¿‡)\n\n")
            f.write(f"**æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d')}\n\n")
            f.write("â„¹ï¸ `SKIP_PREDICTIONS=1` å·²è®¾ç½®ï¼Œæœ¬æ¬¡è¿è¡Œè·³è¿‡äº†æ¨¡å‹é¢„æµ‹å’Œè¯¦ç»†æŠ¥å‘Šç”Ÿæˆã€‚\n")
        print(f"âœ… å·²ç”Ÿæˆå ä½æŠ¥å‘Š: {report_path}", flush=True)
    else:
        if model_enabled and model is not None:
            # æ”¶é›†ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
            missing_features_info = {
                'status': 'æ­£å¸¸',
                'missing': []
            }
            
            if not HAS_FEATURE_ENGINE:
                missing_features_info['status'] = 'ä¸¥é‡é™çº§ (æ— ç‰¹å¾å·¥ç¨‹)'
                missing_features_info['missing'].append("ç‰¹å¾å·¥ç¨‹æ¨¡å— (shared/features.py)")
            
            if fields_daily_basic is None:
                missing_features_info['status'] = 'é™çº§ (ç¼ºå¤±åŸºæœ¬é¢)'
                missing_features_info['missing'].append("åŸºæœ¬é¢æ•°æ® (daily_basic: free_turnover, pe, pb)")
                
            # æ£€æŸ¥æ˜¯å¦æœ‰èµ„é‡‘æµæ•°æ® (é€šè¿‡æ£€æŸ¥ report ä¸­çš„ç‰¹å¾åˆ—ï¼Œæˆ–è€…ç®€å•å‡è®¾å¦‚æœé…ç½®äº†å°±æœ‰)
            # è¿™é‡Œç®€å•æ£€æŸ¥é…ç½®
            if not fields_moneyflow:
                 missing_features_info['missing'].append("èµ„é‡‘æµæ•°æ® (moneyflow)")
            
            generate_report(missing_features_info)

    print("ğŸ‰ RUN_DONE: æ‰€æœ‰ä»»åŠ¡å®Œæˆ", flush=True)

if __name__ == "__main__":
    main()
